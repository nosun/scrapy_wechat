2020-11-21 23:42:17 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: dangdang)
2020-11-21 23:42:17 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'dangdang', 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'LOG_ENABLED': False, 'LOG_FILE': 'logging.log', 'NEWSPIDER_MODULE': 'dangdang.spiders', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['dangdang.spiders'], 'USER_AGENT': 'dangdang (+http://www.yourdomain.com)'}
2020-11-21 23:42:17 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2020-11-21 23:42:17 [book] INFO: Reading start URLs from redis key 'book:start_urls' (batch size: 16, encoding: utf-8
2020-11-21 23:42:17 [scrapy.middleware] INFO: Enabled downloader middlewares:
['dangdang.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'dangdang.middlewares.DangdangDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-11-21 23:42:17 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'dangdang.middlewares.DangdangSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-11-21 23:42:18 [scrapy.middleware] INFO: Enabled item pipelines:
['dangdang.pipelines.DangdangPipeline']
2020-11-21 23:42:18 [scrapy.core.engine] INFO: Spider opened
2020-11-21 23:42:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-11-21 23:42:18 [book] INFO: Spider opened: book
2020-11-21 23:42:18 [book] INFO: Spider opened: book
2020-11-21 23:43:08 [book] DEBUG: Read 1 requests from 'book:start_urls'
2020-11-21 23:43:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/28517345.html> (referer: https://www.dangdang.com)
2020-11-21 23:43:08 [scrapy.core.scraper] DEBUG: Scraped from <200 http://product.dangdang.com/28517345.html>
{}
2020-11-21 23:43:18 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 1 items (at 1 items/min)
2020-11-21 23:43:38 [book] DEBUG: Read 1 requests from 'book:start_urls'
2020-11-21 23:43:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/29116389.html> (referer: https://www.dangdang.com)
2020-11-21 23:43:38 [scrapy.core.scraper] DEBUG: Scraped from <200 http://product.dangdang.com/29116389.html>
{}
2020-11-21 23:44:18 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 1 pages/min), scraped 2 items (at 1 items/min)
2020-11-21 23:45:18 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 2 items (at 0 items/min)
2020-11-21 23:46:03 [book] DEBUG: Read 1 requests from 'book:start_urls'
2020-11-21 23:46:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/28530936.html> (referer: https://www.dangdang.com)
2020-11-21 23:46:03 [scrapy.core.scraper] DEBUG: Scraped from <200 http://product.dangdang.com/28530936.html>
{}
2020-11-21 23:46:18 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 1 pages/min), scraped 3 items (at 1 items/min)
2020-11-21 23:47:18 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 0 pages/min), scraped 3 items (at 0 items/min)
2020-11-21 23:48:18 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 0 pages/min), scraped 3 items (at 0 items/min)
2020-11-21 23:49:18 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 0 pages/min), scraped 3 items (at 0 items/min)
2020-11-21 23:50:18 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 0 pages/min), scraped 3 items (at 0 items/min)
2020-11-21 23:51:18 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 0 pages/min), scraped 3 items (at 0 items/min)
2020-11-21 23:52:18 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 0 pages/min), scraped 3 items (at 0 items/min)
2020-11-21 23:53:18 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 0 pages/min), scraped 3 items (at 0 items/min)
2020-11-21 23:54:18 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 0 pages/min), scraped 3 items (at 0 items/min)
2020-11-21 23:55:18 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 0 pages/min), scraped 3 items (at 0 items/min)
2020-11-21 23:56:18 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 0 pages/min), scraped 3 items (at 0 items/min)
2020-11-21 23:57:18 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 0 pages/min), scraped 3 items (at 0 items/min)
2020-11-21 23:58:18 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 0 pages/min), scraped 3 items (at 0 items/min)
2020-11-21 23:59:18 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 0 pages/min), scraped 3 items (at 0 items/min)
2020-11-21 23:59:46 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2020-11-21 23:59:46 [scrapy.core.engine] INFO: Closing spider (shutdown)
2020-11-21 23:59:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1948,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 98456,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2020, 11, 21, 15, 59, 46, 880743),
 'item_scraped_count': 3,
 'log_count/DEBUG': 9,
 'log_count/INFO': 28,
 'memusage/max': 56463360,
 'memusage/startup': 54509568,
 'response_received_count': 3,
 'scheduler/dequeued/redis': 3,
 'scheduler/enqueued/redis': 3,
 'start_time': datetime.datetime(2020, 11, 21, 15, 42, 18, 119578)}
2020-11-21 23:59:46 [scrapy.core.engine] INFO: Spider closed (shutdown)
2020-11-21 23:59:51 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: dangdang)
2020-11-21 23:59:51 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'dangdang', 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'LOG_ENABLED': False, 'LOG_FILE': 'logging.log', 'NEWSPIDER_MODULE': 'dangdang.spiders', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['dangdang.spiders'], 'USER_AGENT': 'dangdang (+http://www.yourdomain.com)'}
2020-11-21 23:59:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2020-11-21 23:59:51 [book] INFO: Reading start URLs from redis key 'book:start_urls' (batch size: 16, encoding: utf-8
2020-11-21 23:59:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['dangdang.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'dangdang.middlewares.DangdangDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-11-21 23:59:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'dangdang.middlewares.DangdangSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-11-21 23:59:51 [scrapy.middleware] INFO: Enabled item pipelines:
['dangdang.pipelines.DangdangPipeline']
2020-11-21 23:59:51 [scrapy.core.engine] INFO: Spider opened
2020-11-21 23:59:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-11-21 23:59:51 [book] INFO: Spider opened: book
2020-11-21 23:59:51 [book] INFO: Spider opened: book
2020-11-21 23:59:56 [book] DEBUG: Read 1 requests from 'book:start_urls'
2020-11-21 23:59:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/28530936.html> (referer: https://www.dangdang.com)
2020-11-21 23:59:56 [scrapy.core.scraper] DEBUG: Scraped from <200 http://product.dangdang.com/28530936.html>
{}
2020-11-22 00:00:36 [book] DEBUG: Read 1 requests from 'book:start_urls'
2020-11-22 00:00:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/28542598.html> (referer: https://www.dangdang.com)
2020-11-22 00:00:36 [scrapy.core.scraper] DEBUG: Scraped from <200 http://product.dangdang.com/28542598.html>
{}
2020-11-22 00:00:51 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 2 items (at 2 items/min)
2020-11-22 00:01:28 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2020-11-22 00:01:28 [scrapy.core.engine] INFO: Closing spider (shutdown)
2020-11-22 00:01:28 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1246,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 67523,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2020, 11, 21, 16, 1, 28, 259416),
 'item_scraped_count': 2,
 'log_count/DEBUG': 6,
 'log_count/INFO': 12,
 'memusage/max': 56246272,
 'memusage/startup': 54452224,
 'response_received_count': 2,
 'scheduler/dequeued/redis': 2,
 'scheduler/enqueued/redis': 2,
 'start_time': datetime.datetime(2020, 11, 21, 15, 59, 51, 419296)}
2020-11-22 00:01:28 [scrapy.core.engine] INFO: Spider closed (shutdown)
2020-11-22 00:01:33 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: dangdang)
2020-11-22 00:01:33 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'dangdang', 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'LOG_ENABLED': False, 'LOG_FILE': 'logging.log', 'NEWSPIDER_MODULE': 'dangdang.spiders', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['dangdang.spiders'], 'USER_AGENT': 'dangdang (+http://www.yourdomain.com)'}
2020-11-22 00:01:33 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2020-11-22 00:01:33 [book] INFO: Reading start URLs from redis key 'book:start_urls' (batch size: 16, encoding: utf-8
2020-11-22 00:01:33 [scrapy.middleware] INFO: Enabled downloader middlewares:
['dangdang.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'dangdang.middlewares.DangdangDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-11-22 00:01:33 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'dangdang.middlewares.DangdangSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-11-22 00:01:33 [scrapy.middleware] INFO: Enabled item pipelines:
['dangdang.pipelines.DangdangPipeline']
2020-11-22 00:01:33 [scrapy.core.engine] INFO: Spider opened
2020-11-22 00:01:33 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-11-22 00:01:33 [book] INFO: Spider opened: book
2020-11-22 00:01:33 [book] INFO: Spider opened: book
2020-11-22 00:01:38 [book] DEBUG: Read 1 requests from 'book:start_urls'
2020-11-22 00:01:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/28542598.html> (referer: https://www.dangdang.com)
2020-11-22 00:01:39 [scrapy.core.scraper] DEBUG: Scraped from <200 http://product.dangdang.com/28542598.html>
{}
2020-11-22 00:02:33 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 1 items (at 1 items/min)
2020-11-22 00:03:33 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 00:04:33 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 00:05:33 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 00:05:55 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2020-11-22 00:05:55 [scrapy.core.engine] INFO: Closing spider (shutdown)
2020-11-22 00:05:55 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 664,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 33788,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2020, 11, 21, 16, 5, 55, 409667),
 'item_scraped_count': 1,
 'log_count/DEBUG': 3,
 'log_count/INFO': 15,
 'memusage/max': 56197120,
 'memusage/startup': 54603776,
 'response_received_count': 1,
 'scheduler/dequeued/redis': 1,
 'scheduler/enqueued/redis': 1,
 'start_time': datetime.datetime(2020, 11, 21, 16, 1, 33, 808366)}
2020-11-22 00:05:55 [scrapy.core.engine] INFO: Spider closed (shutdown)
2020-11-22 00:05:58 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: dangdang)
2020-11-22 00:05:58 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'dangdang', 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'LOG_ENABLED': False, 'LOG_FILE': 'logging.log', 'NEWSPIDER_MODULE': 'dangdang.spiders', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['dangdang.spiders'], 'USER_AGENT': 'dangdang (+http://www.yourdomain.com)'}
2020-11-22 00:05:58 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2020-11-22 00:05:58 [book] INFO: Reading start URLs from redis key 'book:start_urls' (batch size: 16, encoding: utf-8
2020-11-22 00:05:58 [scrapy.middleware] INFO: Enabled downloader middlewares:
['dangdang.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'dangdang.middlewares.DangdangDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-11-22 00:05:58 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'dangdang.middlewares.DangdangSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-11-22 00:05:58 [scrapy.middleware] INFO: Enabled item pipelines:
['dangdang.pipelines.DangdangPipeline']
2020-11-22 00:05:58 [scrapy.core.engine] INFO: Spider opened
2020-11-22 00:05:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-11-22 00:05:58 [book] INFO: Spider opened: book
2020-11-22 00:05:58 [book] INFO: Spider opened: book
2020-11-22 00:06:08 [book] DEBUG: Read 1 requests from 'book:start_urls'
2020-11-22 00:06:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/28542599.html> (referer: https://www.dangdang.com)
2020-11-22 00:06:08 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.dangdang.com/28542599.html> (referer: https://www.dangdang.com)
Traceback (most recent call last):
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Users/nosun/data/python/scrapy/dangdang/dangdang/middlewares.py", line 39, in process_spider_output
    for i in result:
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/nosun/data/python/scrapy/dangdang/dangdang/spiders/book.py", line 34, in parse
    book.url = url
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/item.py", line 79, in __setattr__
    (name, value))
AttributeError: Use item['url'] = 'http://product.dangdang.com/28542599.html' to set field value
2020-11-22 00:06:42 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2020-11-22 00:06:42 [scrapy.core.engine] INFO: Closing spider (shutdown)
2020-11-22 00:06:42 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 639,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 33775,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2020, 11, 21, 16, 6, 42, 631744),
 'log_count/DEBUG': 2,
 'log_count/ERROR': 1,
 'log_count/INFO': 11,
 'memusage/max': 54476800,
 'memusage/startup': 54476800,
 'response_received_count': 1,
 'scheduler/dequeued/redis': 1,
 'scheduler/enqueued/redis': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2020, 11, 21, 16, 5, 58, 298480)}
2020-11-22 00:06:42 [scrapy.core.engine] INFO: Spider closed (shutdown)
2020-11-22 00:07:05 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: dangdang)
2020-11-22 00:07:05 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'dangdang', 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'LOG_ENABLED': False, 'LOG_FILE': 'logging.log', 'NEWSPIDER_MODULE': 'dangdang.spiders', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['dangdang.spiders'], 'USER_AGENT': 'dangdang (+http://www.yourdomain.com)'}
2020-11-22 00:07:05 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2020-11-22 00:07:05 [book] INFO: Reading start URLs from redis key 'book:start_urls' (batch size: 16, encoding: utf-8
2020-11-22 00:07:05 [scrapy.middleware] INFO: Enabled downloader middlewares:
['dangdang.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'dangdang.middlewares.DangdangDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-11-22 00:07:05 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'dangdang.middlewares.DangdangSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-11-22 00:07:05 [scrapy.middleware] INFO: Enabled item pipelines:
['dangdang.pipelines.DangdangPipeline']
2020-11-22 00:07:05 [scrapy.core.engine] INFO: Spider opened
2020-11-22 00:07:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-11-22 00:07:05 [book] INFO: Spider opened: book
2020-11-22 00:07:05 [book] INFO: Spider opened: book
2020-11-22 00:07:10 [book] DEBUG: Read 1 requests from 'book:start_urls'
2020-11-22 00:07:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/28542599.html> (referer: https://www.dangdang.com)
2020-11-22 00:07:10 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.dangdang.com/28542599.html> (referer: https://www.dangdang.com)
Traceback (most recent call last):
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Users/nosun/data/python/scrapy/dangdang/dangdang/middlewares.py", line 39, in process_spider_output
    for i in result:
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/nosun/data/python/scrapy/dangdang/dangdang/spiders/book.py", line 34, in parse
    book.url = url
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/item.py", line 79, in __setattr__
    (name, value))
AttributeError: Use item['url'] = 'http://product.dangdang.com/28542599.html' to set field value
2020-11-22 00:07:15 [book] DEBUG: Read 1 requests from 'book:start_urls'
2020-11-22 00:07:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/28542598.html> (referer: https://www.dangdang.com)
2020-11-22 00:07:15 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.dangdang.com/28542598.html> (referer: https://www.dangdang.com)
Traceback (most recent call last):
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Users/nosun/data/python/scrapy/dangdang/dangdang/middlewares.py", line 39, in process_spider_output
    for i in result:
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/nosun/data/python/scrapy/dangdang/dangdang/spiders/book.py", line 34, in parse
    book.url = url
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/item.py", line 79, in __setattr__
    (name, value))
AttributeError: Use item['url'] = 'http://product.dangdang.com/28542598.html' to set field value
2020-11-22 00:07:45 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2020-11-22 00:07:45 [scrapy.core.engine] INFO: Closing spider (shutdown)
2020-11-22 00:07:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1216,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 67562,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2020, 11, 21, 16, 7, 45, 847647),
 'log_count/DEBUG': 4,
 'log_count/ERROR': 2,
 'log_count/INFO': 11,
 'memusage/max': 54546432,
 'memusage/startup': 54546432,
 'response_received_count': 2,
 'scheduler/dequeued/redis': 2,
 'scheduler/enqueued/redis': 2,
 'spider_exceptions/AttributeError': 2,
 'start_time': datetime.datetime(2020, 11, 21, 16, 7, 5, 359821)}
2020-11-22 00:07:45 [scrapy.core.engine] INFO: Spider closed (shutdown)
2020-11-22 00:07:50 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: dangdang)
2020-11-22 00:07:50 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'dangdang', 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'LOG_ENABLED': False, 'LOG_FILE': 'logging.log', 'NEWSPIDER_MODULE': 'dangdang.spiders', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['dangdang.spiders'], 'USER_AGENT': 'dangdang (+http://www.yourdomain.com)'}
2020-11-22 00:07:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2020-11-22 00:07:50 [book] INFO: Reading start URLs from redis key 'book:start_urls' (batch size: 16, encoding: utf-8
2020-11-22 00:07:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['dangdang.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'dangdang.middlewares.DangdangDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-11-22 00:07:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'dangdang.middlewares.DangdangSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-11-22 00:07:50 [scrapy.middleware] INFO: Enabled item pipelines:
['dangdang.pipelines.DangdangPipeline']
2020-11-22 00:07:50 [scrapy.core.engine] INFO: Spider opened
2020-11-22 00:07:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-11-22 00:07:50 [book] INFO: Spider opened: book
2020-11-22 00:07:50 [book] INFO: Spider opened: book
2020-11-22 00:08:15 [book] DEBUG: Read 1 requests from 'book:start_urls'
2020-11-22 00:08:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/29116323.html> (referer: https://www.dangdang.com)
2020-11-22 00:08:15 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.dangdang.com/29116323.html> (referer: https://www.dangdang.com)
Traceback (most recent call last):
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Users/nosun/data/python/scrapy/dangdang/dangdang/middlewares.py", line 39, in process_spider_output
    for i in result:
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/nosun/data/python/scrapy/dangdang/dangdang/spiders/book.py", line 34, in parse
    book.url = url
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/item.py", line 79, in __setattr__
    (name, value))
AttributeError: Use item['url'] = 'http://product.dangdang.com/29116323.html' to set field value
2020-11-22 00:08:50 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2020-11-22 00:09:50 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-11-22 00:10:01 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2020-11-22 00:10:01 [scrapy.core.engine] INFO: Closing spider (shutdown)
2020-11-22 00:10:01 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 589,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 31484,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2020, 11, 21, 16, 10, 1, 873820),
 'log_count/DEBUG': 2,
 'log_count/ERROR': 1,
 'log_count/INFO': 13,
 'memusage/max': 56143872,
 'memusage/startup': 54575104,
 'response_received_count': 1,
 'scheduler/dequeued/redis': 1,
 'scheduler/enqueued/redis': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2020, 11, 21, 16, 7, 50, 299669)}
2020-11-22 00:10:01 [scrapy.core.engine] INFO: Spider closed (shutdown)
2020-11-22 00:10:04 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: dangdang)
2020-11-22 00:10:04 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'dangdang', 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'LOG_ENABLED': False, 'LOG_FILE': 'logging.log', 'NEWSPIDER_MODULE': 'dangdang.spiders', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['dangdang.spiders'], 'USER_AGENT': 'dangdang (+http://www.yourdomain.com)'}
2020-11-22 00:10:04 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2020-11-22 00:10:04 [book] INFO: Reading start URLs from redis key 'book:start_urls' (batch size: 16, encoding: utf-8
2020-11-22 00:10:04 [scrapy.middleware] INFO: Enabled downloader middlewares:
['dangdang.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'dangdang.middlewares.DangdangDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-11-22 00:10:04 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'dangdang.middlewares.DangdangSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-11-22 00:10:04 [scrapy.middleware] INFO: Enabled item pipelines:
['dangdang.pipelines.DangdangPipeline']
2020-11-22 00:10:04 [scrapy.core.engine] INFO: Spider opened
2020-11-22 00:10:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-11-22 00:10:04 [book] INFO: Spider opened: book
2020-11-22 00:10:04 [book] INFO: Spider opened: book
2020-11-22 00:10:09 [book] DEBUG: Read 1 requests from 'book:start_urls'
2020-11-22 00:10:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/28542598.html> (referer: https://www.dangdang.com)
2020-11-22 00:10:10 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.dangdang.com/28542598.html> (referer: https://www.dangdang.com)
Traceback (most recent call last):
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Users/nosun/data/python/scrapy/dangdang/dangdang/middlewares.py", line 39, in process_spider_output
    for i in result:
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/nosun/data/python/scrapy/dangdang/dangdang/spiders/book.py", line 34, in parse
    book.url = url
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/item.py", line 79, in __setattr__
    (name, value))
AttributeError: Use item['url'] = 'http://product.dangdang.com/28542598.html' to set field value
2020-11-22 00:10:34 [book] DEBUG: Read 1 requests from 'book:start_urls'
2020-11-22 00:10:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/24144397.html> (referer: https://www.dangdang.com)
2020-11-22 00:10:35 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.dangdang.com/24144397.html> (referer: https://www.dangdang.com)
Traceback (most recent call last):
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Users/nosun/data/python/scrapy/dangdang/dangdang/middlewares.py", line 39, in process_spider_output
    for i in result:
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/nosun/data/python/scrapy/dangdang/dangdang/spiders/book.py", line 34, in parse
    book.url = url
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/item.py", line 79, in __setattr__
    (name, value))
AttributeError: Use item['url'] = 'http://product.dangdang.com/24144397.html' to set field value
2020-11-22 00:10:52 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2020-11-22 00:10:52 [scrapy.core.engine] INFO: Closing spider (shutdown)
2020-11-22 00:10:52 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1343,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 65050,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2020, 11, 21, 16, 10, 52, 829497),
 'log_count/DEBUG': 4,
 'log_count/ERROR': 2,
 'log_count/INFO': 11,
 'memusage/max': 54587392,
 'memusage/startup': 54587392,
 'response_received_count': 2,
 'scheduler/dequeued/redis': 2,
 'scheduler/enqueued/redis': 2,
 'spider_exceptions/AttributeError': 2,
 'start_time': datetime.datetime(2020, 11, 21, 16, 10, 4, 752624)}
2020-11-22 00:10:52 [scrapy.core.engine] INFO: Spider closed (shutdown)
2020-11-22 00:10:55 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: dangdang)
2020-11-22 00:10:55 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'dangdang', 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'LOG_ENABLED': False, 'LOG_FILE': 'logging.log', 'NEWSPIDER_MODULE': 'dangdang.spiders', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['dangdang.spiders'], 'USER_AGENT': 'dangdang (+http://www.yourdomain.com)'}
2020-11-22 00:10:55 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2020-11-22 00:10:55 [book] INFO: Reading start URLs from redis key 'book:start_urls' (batch size: 16, encoding: utf-8
2020-11-22 00:10:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['dangdang.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'dangdang.middlewares.DangdangDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-11-22 00:10:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'dangdang.middlewares.DangdangSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-11-22 00:10:55 [scrapy.middleware] INFO: Enabled item pipelines:
['dangdang.pipelines.DangdangPipeline']
2020-11-22 00:10:55 [scrapy.core.engine] INFO: Spider opened
2020-11-22 00:10:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-11-22 00:10:55 [book] INFO: Spider opened: book
2020-11-22 00:10:55 [book] INFO: Spider opened: book
2020-11-22 00:11:00 [book] DEBUG: Read 1 requests from 'book:start_urls'
2020-11-22 00:11:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/24144397.html> (referer: https://www.dangdang.com)
2020-11-22 00:11:00 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.dangdang.com/24144397.html> (referer: https://www.dangdang.com)
Traceback (most recent call last):
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Users/nosun/data/python/scrapy/dangdang/dangdang/middlewares.py", line 39, in process_spider_output
    for i in result:
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/nosun/data/python/scrapy/dangdang/dangdang/spiders/book.py", line 34, in parse
    book.url = url
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/item.py", line 79, in __setattr__
    (name, value))
AttributeError: Use item['url'] = 'http://product.dangdang.com/24144397.html' to set field value
2020-11-22 00:11:25 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2020-11-22 00:11:25 [scrapy.core.engine] INFO: Closing spider (shutdown)
2020-11-22 00:11:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 664,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 31264,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2020, 11, 21, 16, 11, 25, 804335),
 'log_count/DEBUG': 2,
 'log_count/ERROR': 1,
 'log_count/INFO': 11,
 'memusage/max': 54636544,
 'memusage/startup': 54636544,
 'response_received_count': 1,
 'scheduler/dequeued/redis': 1,
 'scheduler/enqueued/redis': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2020, 11, 21, 16, 10, 55, 155067)}
2020-11-22 00:11:25 [scrapy.core.engine] INFO: Spider closed (shutdown)
2020-11-22 00:11:27 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: dangdang)
2020-11-22 00:11:27 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'dangdang', 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'LOG_ENABLED': False, 'LOG_FILE': 'logging.log', 'NEWSPIDER_MODULE': 'dangdang.spiders', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['dangdang.spiders'], 'USER_AGENT': 'dangdang (+http://www.yourdomain.com)'}
2020-11-22 00:11:27 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2020-11-22 00:11:27 [book] INFO: Reading start URLs from redis key 'book:start_urls' (batch size: 16, encoding: utf-8
2020-11-22 00:11:27 [scrapy.middleware] INFO: Enabled downloader middlewares:
['dangdang.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'dangdang.middlewares.DangdangDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-11-22 00:11:27 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'dangdang.middlewares.DangdangSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-11-22 00:11:27 [scrapy.middleware] INFO: Enabled item pipelines:
['dangdang.pipelines.DangdangPipeline']
2020-11-22 00:11:27 [scrapy.core.engine] INFO: Spider opened
2020-11-22 00:11:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-11-22 00:11:27 [book] INFO: Spider opened: book
2020-11-22 00:11:27 [book] INFO: Spider opened: book
2020-11-22 00:12:07 [book] DEBUG: Read 1 requests from 'book:start_urls'
2020-11-22 00:12:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/24008607.html> (referer: https://www.dangdang.com)
2020-11-22 00:12:08 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.dangdang.com/24008607.html> (referer: https://www.dangdang.com)
Traceback (most recent call last):
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Users/nosun/data/python/scrapy/dangdang/dangdang/middlewares.py", line 39, in process_spider_output
    for i in result:
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/nosun/data/python/scrapy/dangdang/dangdang/spiders/book.py", line 35, in parse
    book.url = url
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/item.py", line 79, in __setattr__
    (name, value))
AttributeError: Use item['url'] = 'http://product.dangdang.com/24008607.html' to set field value
2020-11-22 00:12:27 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2020-11-22 00:13:12 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2020-11-22 00:13:12 [scrapy.core.engine] INFO: Closing spider (shutdown)
2020-11-22 00:13:12 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 648,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 31505,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2020, 11, 21, 16, 13, 12, 639104),
 'log_count/DEBUG': 2,
 'log_count/ERROR': 1,
 'log_count/INFO': 12,
 'memusage/max': 56098816,
 'memusage/startup': 54550528,
 'response_received_count': 1,
 'scheduler/dequeued/redis': 1,
 'scheduler/enqueued/redis': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2020, 11, 21, 16, 11, 27, 978488)}
2020-11-22 00:13:12 [scrapy.core.engine] INFO: Spider closed (shutdown)
2020-11-22 00:13:14 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: dangdang)
2020-11-22 00:13:14 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'dangdang', 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'LOG_ENABLED': False, 'LOG_FILE': 'logging.log', 'NEWSPIDER_MODULE': 'dangdang.spiders', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['dangdang.spiders'], 'USER_AGENT': 'dangdang (+http://www.yourdomain.com)'}
2020-11-22 00:13:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2020-11-22 00:13:14 [book] INFO: Reading start URLs from redis key 'book:start_urls' (batch size: 16, encoding: utf-8
2020-11-22 00:13:15 [scrapy.middleware] INFO: Enabled downloader middlewares:
['dangdang.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'dangdang.middlewares.DangdangDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-11-22 00:13:15 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'dangdang.middlewares.DangdangSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-11-22 00:13:15 [scrapy.middleware] INFO: Enabled item pipelines:
['dangdang.pipelines.DangdangPipeline']
2020-11-22 00:13:15 [scrapy.core.engine] INFO: Spider opened
2020-11-22 00:13:15 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-11-22 00:13:15 [book] INFO: Spider opened: book
2020-11-22 00:13:15 [book] INFO: Spider opened: book
2020-11-22 00:13:20 [book] DEBUG: Read 1 requests from 'book:start_urls'
2020-11-22 00:13:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/24008607.html> (referer: https://www.dangdang.com)
2020-11-22 00:13:20 [scrapy.core.scraper] DEBUG: Scraped from <200 http://product.dangdang.com/24008607.html>
{'_id': '24008607', 'url': 'http://product.dangdang.com/24008607.html'}
2020-11-22 00:14:15 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 1 items (at 1 items/min)
2020-11-22 00:23:37 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 01:00:37 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 01:10:17 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 01:12:25 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 01:13:15 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 01:14:15 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 01:15:15 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 01:16:15 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 01:16:27 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2020-11-22 01:16:27 [scrapy.core.engine] INFO: Closing spider (shutdown)
2020-11-22 01:16:27 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 657,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 31510,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2020, 11, 21, 17, 16, 27, 671023),
 'item_scraped_count': 1,
 'log_count/DEBUG': 3,
 'log_count/INFO': 20,
 'memusage/max': 56041472,
 'memusage/startup': 54476800,
 'response_received_count': 1,
 'scheduler/dequeued/redis': 1,
 'scheduler/enqueued/redis': 1,
 'start_time': datetime.datetime(2020, 11, 21, 16, 13, 15, 62316)}
2020-11-22 01:16:27 [scrapy.core.engine] INFO: Spider closed (shutdown)
2020-11-22 01:17:48 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: dangdang)
2020-11-22 01:17:48 [scrapy.utils.log] INFO: Versions: lxml 4.4.2.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.7 (default, Mar 10 2020, 15:43:03) - [Clang 11.0.0 (clang-1100.0.33.17)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1h  22 Sep 2020), cryptography 3.2.1, Platform Darwin-19.6.0-x86_64-i386-64bit
2020-11-22 01:17:48 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'dangdang', 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'LOGSTATS_INTERVAL': 0, 'LOG_ENABLED': False, 'LOG_FILE': 'logging.log', 'NEWSPIDER_MODULE': 'dangdang.spiders', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['dangdang.spiders'], 'USER_AGENT': 'dangdang (+http://www.yourdomain.com)'}
2020-11-22 01:17:48 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage']
2020-11-22 01:17:48 [scrapy.middleware] INFO: Enabled downloader middlewares:
['dangdang.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'dangdang.middlewares.DangdangDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-11-22 01:17:48 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'dangdang.middlewares.DangdangSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-11-22 01:17:50 [scrapy.middleware] INFO: Enabled item pipelines:
['dangdang.pipelines.DangdangPipeline']
2020-11-22 01:18:35 [scrapy.core.engine] INFO: Spider opened
2020-11-22 01:18:35 [default] INFO: Spider opened: default
2020-11-22 01:18:35 [default] INFO: Spider opened: default
2020-11-22 01:54:37 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: dangdang)
2020-11-22 01:54:37 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'dangdang', 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_ENABLED': False, 'LOG_FILE': 'logging.log', 'NEWSPIDER_MODULE': 'dangdang.spiders', 'SPIDER_MODULES': ['dangdang.spiders'], 'USER_AGENT': 'dangdang (+http://www.yourdomain.com)'}
2020-11-22 01:54:37 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2020-11-22 01:54:37 [book] INFO: Reading start URLs from redis key 'book:start_urls' (batch size: 16, encoding: utf-8
2020-11-22 01:54:37 [scrapy.middleware] INFO: Enabled downloader middlewares:
['dangdang.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'dangdang.middlewares.DangdangDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-11-22 01:54:37 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'dangdang.middlewares.DangdangSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-11-22 01:54:37 [scrapy.middleware] INFO: Enabled item pipelines:
['dangdang.pipelines.DangdangPipeline']
2020-11-22 01:54:37 [scrapy.core.engine] INFO: Spider opened
2020-11-22 01:54:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-11-22 01:54:37 [book] INFO: Spider opened: book
2020-11-22 01:54:37 [book] INFO: Spider opened: book
2020-11-22 01:54:57 [book] DEBUG: Read 1 requests from 'book:start_urls'
2020-11-22 01:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/23417701.html> (referer: https://www.dangdang.com)
2020-11-22 01:54:57 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.dangdang.com/23417701.html> (referer: https://www.dangdang.com)
Traceback (most recent call last):
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Users/nosun/data/python/scrapy/dangdang/dangdang/middlewares.py", line 39, in process_spider_output
    for i in result:
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/nosun/data/python/scrapy/dangdang/dangdang/spiders/book.py", line 58, in parse
    book_detail.css('li#detail-category-path>span.lie').xpath('a/text()').extract())
  File "/Users/nosun/data/python/scrapy/dangdang/dangdang/spiders/book.py", line 68, in format_book_detail
    arr = data.split('：')
AttributeError: 'list' object has no attribute 'split'
2020-11-22 01:55:37 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2020-11-22 01:56:05 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2020-11-22 01:56:05 [scrapy.core.engine] INFO: Closing spider (shutdown)
2020-11-22 01:56:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 664,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 30073,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2020, 11, 21, 17, 56, 5, 240775),
 'log_count/DEBUG': 2,
 'log_count/ERROR': 1,
 'log_count/INFO': 12,
 'memusage/max': 57282560,
 'memusage/startup': 54628352,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2020, 11, 21, 17, 54, 37, 194027)}
2020-11-22 01:56:05 [scrapy.core.engine] INFO: Spider closed (shutdown)
2020-11-22 01:56:09 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: dangdang)
2020-11-22 01:56:09 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'dangdang', 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'LOG_ENABLED': False, 'LOG_FILE': 'logging.log', 'NEWSPIDER_MODULE': 'dangdang.spiders', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['dangdang.spiders'], 'USER_AGENT': 'dangdang (+http://www.yourdomain.com)'}
2020-11-22 01:56:09 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2020-11-22 01:56:09 [book] INFO: Reading start URLs from redis key 'book:start_urls' (batch size: 16, encoding: utf-8
2020-11-22 01:56:09 [scrapy.middleware] INFO: Enabled downloader middlewares:
['dangdang.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'dangdang.middlewares.DangdangDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-11-22 01:56:09 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'dangdang.middlewares.DangdangSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-11-22 01:56:09 [scrapy.middleware] INFO: Enabled item pipelines:
['dangdang.pipelines.DangdangPipeline']
2020-11-22 01:56:09 [scrapy.core.engine] INFO: Spider opened
2020-11-22 01:56:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-11-22 01:56:09 [book] INFO: Spider opened: book
2020-11-22 01:56:09 [book] INFO: Spider opened: book
2020-11-22 01:56:14 [book] DEBUG: Read 1 requests from 'book:start_urls'
2020-11-22 01:56:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/23417701.html> (referer: https://www.dangdang.com)
2020-11-22 01:56:14 [scrapy.core.scraper] DEBUG: Scraped from <200 http://product.dangdang.com/23417701.html>
{'_id': '23417701',
 'isbn': '23417701',
 'name': '0-4岁幼儿认知小百科（全3册)',
 'package': '平装-胶订',
 'paper': '胶版纸',
 'size': '24开',
 'suit': '否',
 'url': 'http://product.dangdang.com/23417701.html'}
2020-11-22 01:57:09 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 1 items (at 1 items/min)
2020-11-22 02:06:47 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 02:07:09 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 02:54:21 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 03:04:02 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 03:04:09 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 04:54:38 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 08:15:02 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 08:15:09 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 10:06:30 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 10:07:09 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 10:08:09 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 10:09:09 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 10:10:09 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 10:11:09 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 10:12:09 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 10:13:09 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 10:14:09 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 10:15:09 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 10:16:09 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 10:17:09 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 10:18:09 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 10:19:09 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 10:20:09 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 10:47:50 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 10:48:09 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 10:49:09 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 10:50:09 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 10:51:09 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 10:52:03 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2020-11-22 10:52:03 [scrapy.core.engine] INFO: Closing spider (shutdown)
2020-11-22 10:52:03 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 648,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 30071,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2020, 11, 22, 2, 52, 3, 530507),
 'item_scraped_count': 1,
 'log_count/DEBUG': 3,
 'log_count/INFO': 40,
 'memusage/max': 57331712,
 'memusage/startup': 54697984,
 'response_received_count': 1,
 'scheduler/dequeued/redis': 1,
 'scheduler/enqueued/redis': 1,
 'start_time': datetime.datetime(2020, 11, 21, 17, 56, 9, 320805)}
2020-11-22 10:52:03 [scrapy.core.engine] INFO: Spider closed (shutdown)
2020-11-22 10:52:23 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: dangdang)
2020-11-22 10:52:23 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'dangdang', 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0, 'LOG_ENABLED': False, 'LOG_FILE': 'logging.log', 'NEWSPIDER_MODULE': 'dangdang.spiders', 'SPIDER_MODULES': ['dangdang.spiders'], 'USER_AGENT': 'dangdang (+http://www.yourdomain.com)'}
2020-11-22 10:52:23 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage']
2020-11-22 10:52:23 [scrapy.middleware] INFO: Enabled downloader middlewares:
['dangdang.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'dangdang.middlewares.DangdangDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-11-22 10:52:23 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'dangdang.middlewares.DangdangSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-11-22 10:52:23 [scrapy.middleware] INFO: Enabled item pipelines:
['dangdang.pipelines.DangdangPipeline']
2020-11-22 10:52:36 [scrapy.core.engine] INFO: Spider opened
2020-11-22 10:52:36 [default] INFO: Spider opened: default
2020-11-22 10:52:36 [default] INFO: Spider opened: default
2020-11-22 10:52:36 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://product.dangdang.com/23417701.html> (failed 1 times): DNS lookup failed: no results for hostname lookup: product.dangdang.com.
2020-11-22 10:52:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://product.dangdang.com/23417701.html> (failed 2 times): DNS lookup failed: no results for hostname lookup: product.dangdang.com.
2020-11-22 10:52:38 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://product.dangdang.com/23417701.html> (failed 3 times): DNS lookup failed: no results for hostname lookup: product.dangdang.com.
2020-11-22 21:39:15 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: dangdang)
2020-11-22 21:39:15 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'dangdang', 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_ENABLED': False, 'LOG_FILE': 'logging.log', 'NEWSPIDER_MODULE': 'dangdang.spiders', 'SPIDER_MODULES': ['dangdang.spiders'], 'USER_AGENT': 'dangdang (+http://www.yourdomain.com)'}
2020-11-22 21:39:15 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2020-11-22 21:39:15 [book] INFO: Reading start URLs from redis key 'book:start_urls' (batch size: 16, encoding: utf-8
2020-11-22 21:39:15 [scrapy.middleware] INFO: Enabled downloader middlewares:
['dangdang.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'dangdang.middlewares.DangdangDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-11-22 21:39:15 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'dangdang.middlewares.DangdangSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-11-22 21:39:15 [scrapy.middleware] INFO: Enabled item pipelines:
['dangdang.pipelines.DangdangPipeline']
2020-11-22 21:39:15 [scrapy.core.engine] INFO: Spider opened
2020-11-22 21:39:15 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-11-22 21:39:15 [book] INFO: Spider opened: book
2020-11-22 21:39:15 [book] INFO: Spider opened: book
2020-11-22 21:39:42 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2020-11-22 21:39:42 [scrapy.core.engine] INFO: Closing spider (shutdown)
2020-11-22 21:39:42 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2020, 11, 22, 13, 39, 42, 949722),
 'log_count/INFO': 11,
 'memusage/max': 54726656,
 'memusage/startup': 54726656,
 'start_time': datetime.datetime(2020, 11, 22, 13, 39, 15, 951531)}
2020-11-22 21:39:42 [scrapy.core.engine] INFO: Spider closed (shutdown)
2020-11-22 21:40:28 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: dangdang)
2020-11-22 21:40:28 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'dangdang', 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'LOG_ENABLED': False, 'LOG_FILE': 'logging.log', 'NEWSPIDER_MODULE': 'dangdang.spiders', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['dangdang.spiders'], 'USER_AGENT': 'dangdang (+http://www.yourdomain.com)'}
2020-11-22 21:40:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2020-11-22 21:40:28 [book] INFO: Reading start URLs from redis key 'book:start_urls' (batch size: 16, encoding: utf-8
2020-11-22 21:40:28 [scrapy.middleware] INFO: Enabled downloader middlewares:
['dangdang.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'dangdang.middlewares.DangdangDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-11-22 21:40:28 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'dangdang.middlewares.DangdangSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-11-22 21:40:28 [scrapy.middleware] INFO: Enabled item pipelines:
['dangdang.pipelines.DangdangPipeline']
2020-11-22 21:40:28 [scrapy.core.engine] INFO: Spider opened
2020-11-22 21:40:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-11-22 21:40:28 [book] INFO: Spider opened: book
2020-11-22 21:40:28 [book] INFO: Spider opened: book
2020-11-22 21:40:33 [book] DEBUG: Read 1 requests from 'book:start_urls'
2020-11-22 21:40:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/23417701.html> (referer: https://www.dangdang.com)
2020-11-22 21:40:34 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.dangdang.com/23417701.html> (referer: https://www.dangdang.com)
Traceback (most recent call last):
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Users/nosun/data/python/scrapy/dangdang/dangdang/middlewares.py", line 39, in process_spider_output
    for i in result:
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/nosun/data/python/scrapy/dangdang/dangdang/spiders/book.py", line 22, in parse
    book['author'], book['painter'], book['translator'], book['maker'] = self.parse_author_info(response)
  File "/Users/nosun/data/python/scrapy/dangdang/dangdang/spiders/book.py", line 141, in parse_author_info
    for k, v in type_list:
ValueError: too many values to unpack (expected 2)
2020-11-22 21:41:28 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2020-11-22 21:42:28 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-11-22 21:43:28 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-11-22 21:44:28 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-11-22 21:45:10 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2020-11-22 21:45:10 [scrapy.core.engine] INFO: Closing spider (shutdown)
2020-11-22 21:45:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 625,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 30071,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2020, 11, 22, 13, 45, 10, 552009),
 'log_count/DEBUG': 2,
 'log_count/ERROR': 1,
 'log_count/INFO': 15,
 'memusage/max': 57315328,
 'memusage/startup': 54661120,
 'response_received_count': 1,
 'scheduler/dequeued/redis': 1,
 'scheduler/enqueued/redis': 1,
 'spider_exceptions/ValueError': 1,
 'start_time': datetime.datetime(2020, 11, 22, 13, 40, 28, 698001)}
2020-11-22 21:45:10 [scrapy.core.engine] INFO: Spider closed (shutdown)
2020-11-22 21:45:12 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: dangdang)
2020-11-22 21:45:12 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'dangdang', 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'LOG_ENABLED': False, 'LOG_FILE': 'logging.log', 'NEWSPIDER_MODULE': 'dangdang.spiders', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['dangdang.spiders'], 'USER_AGENT': 'dangdang (+http://www.yourdomain.com)'}
2020-11-22 21:45:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2020-11-22 21:45:12 [book] INFO: Reading start URLs from redis key 'book:start_urls' (batch size: 16, encoding: utf-8
2020-11-22 21:45:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['dangdang.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'dangdang.middlewares.DangdangDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-11-22 21:45:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'dangdang.middlewares.DangdangSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-11-22 21:45:12 [scrapy.middleware] INFO: Enabled item pipelines:
['dangdang.pipelines.DangdangPipeline']
2020-11-22 21:45:12 [scrapy.core.engine] INFO: Spider opened
2020-11-22 21:45:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-11-22 21:45:12 [book] INFO: Spider opened: book
2020-11-22 21:45:12 [book] INFO: Spider opened: book
2020-11-22 21:45:17 [book] DEBUG: Read 1 requests from 'book:start_urls'
2020-11-22 21:45:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/23417701.html> (referer: https://www.dangdang.com)
2020-11-22 21:45:17 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.dangdang.com/23417701.html> (referer: https://www.dangdang.com)
Traceback (most recent call last):
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Users/nosun/data/python/scrapy/dangdang/dangdang/middlewares.py", line 39, in process_spider_output
    for i in result:
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/nosun/data/python/scrapy/dangdang/dangdang/spiders/book.py", line 23, in parse
    book['author'], book['painter'], book['translator'], book['maker'] = self.parse_author_info(response)
  File "/Users/nosun/data/python/scrapy/dangdang/dangdang/spiders/book.py", line 151, in parse_author_info
    maker = author_list[k]
IndexError: list index out of range
2020-11-22 21:46:03 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2020-11-22 21:46:03 [scrapy.core.engine] INFO: Closing spider (shutdown)
2020-11-22 21:46:03 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 688,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 30070,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2020, 11, 22, 13, 46, 3, 109961),
 'log_count/DEBUG': 2,
 'log_count/ERROR': 1,
 'log_count/INFO': 11,
 'memusage/max': 54726656,
 'memusage/startup': 54726656,
 'response_received_count': 1,
 'scheduler/dequeued/redis': 1,
 'scheduler/enqueued/redis': 1,
 'spider_exceptions/IndexError': 1,
 'start_time': datetime.datetime(2020, 11, 22, 13, 45, 12, 573249)}
2020-11-22 21:46:03 [scrapy.core.engine] INFO: Spider closed (shutdown)
2020-11-22 21:46:16 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: dangdang)
2020-11-22 21:46:16 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'dangdang', 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'LOG_ENABLED': False, 'LOG_FILE': 'logging.log', 'NEWSPIDER_MODULE': 'dangdang.spiders', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['dangdang.spiders'], 'USER_AGENT': 'dangdang (+http://www.yourdomain.com)'}
2020-11-22 21:46:16 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2020-11-22 21:46:16 [book] INFO: Reading start URLs from redis key 'book:start_urls' (batch size: 16, encoding: utf-8
2020-11-22 21:46:16 [scrapy.middleware] INFO: Enabled downloader middlewares:
['dangdang.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'dangdang.middlewares.DangdangDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-11-22 21:46:16 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'dangdang.middlewares.DangdangSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-11-22 21:46:17 [scrapy.middleware] INFO: Enabled item pipelines:
['dangdang.pipelines.DangdangPipeline']
2020-11-22 21:46:17 [scrapy.core.engine] INFO: Spider opened
2020-11-22 21:46:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-11-22 21:46:17 [book] INFO: Spider opened: book
2020-11-22 21:46:17 [book] INFO: Spider opened: book
2020-11-22 21:46:42 [book] DEBUG: Read 1 requests from 'book:start_urls'
2020-11-22 21:46:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/23417701.html> (referer: https://www.dangdang.com)
2020-11-22 21:48:00 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2020-11-22 21:52:55 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: dangdang)
2020-11-22 21:52:55 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'dangdang', 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0, 'LOG_ENABLED': False, 'LOG_FILE': 'logging.log', 'NEWSPIDER_MODULE': 'dangdang.spiders', 'SPIDER_MODULES': ['dangdang.spiders'], 'USER_AGENT': 'dangdang (+http://www.yourdomain.com)'}
2020-11-22 21:52:55 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage']
2020-11-22 21:52:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['dangdang.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'dangdang.middlewares.DangdangDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-11-22 21:52:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'dangdang.middlewares.DangdangSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-11-22 21:52:55 [scrapy.middleware] INFO: Enabled item pipelines:
['dangdang.pipelines.DangdangPipeline']
2020-11-22 21:53:05 [scrapy.core.engine] INFO: Spider opened
2020-11-22 21:53:05 [default] INFO: Spider opened: default
2020-11-22 21:53:05 [default] INFO: Spider opened: default
2020-11-22 21:53:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/29116389.html> (referer: https://www.dangdang.com)
2020-11-22 21:55:43 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: dangdang)
2020-11-22 21:55:43 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'dangdang', 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'LOG_ENABLED': False, 'LOG_FILE': 'logging.log', 'NEWSPIDER_MODULE': 'dangdang.spiders', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['dangdang.spiders'], 'USER_AGENT': 'dangdang (+http://www.yourdomain.com)'}
2020-11-22 21:55:43 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2020-11-22 21:55:43 [book] INFO: Reading start URLs from redis key 'book:start_urls' (batch size: 16, encoding: utf-8
2020-11-22 21:55:43 [scrapy.middleware] INFO: Enabled downloader middlewares:
['dangdang.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'dangdang.middlewares.DangdangDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-11-22 21:55:43 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'dangdang.middlewares.DangdangSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-11-22 21:55:43 [scrapy.middleware] INFO: Enabled item pipelines:
['dangdang.pipelines.DangdangPipeline']
2020-11-22 21:55:43 [scrapy.core.engine] INFO: Spider opened
2020-11-22 21:55:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-11-22 21:55:43 [book] INFO: Spider opened: book
2020-11-22 21:55:43 [book] INFO: Spider opened: book
2020-11-22 21:55:48 [book] DEBUG: Read 1 requests from 'book:start_urls'
2020-11-22 21:55:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/23417701.html> (referer: https://www.dangdang.com)
2020-11-22 21:55:48 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.dangdang.com/23417701.html> (referer: https://www.dangdang.com)
Traceback (most recent call last):
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Users/nosun/data/python/scrapy/dangdang/dangdang/middlewares.py", line 39, in process_spider_output
    for i in result:
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/nosun/data/python/scrapy/dangdang/dangdang/spiders/book.py", line 26, in parse
    book['description'] = self.get_description(response)
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/item.py", line 66, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'BookItem does not support field: description'
2020-11-22 21:56:43 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2020-11-22 21:57:11 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2020-11-22 21:57:11 [scrapy.core.engine] INFO: Closing spider (shutdown)
2020-11-22 21:57:11 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 720,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 30071,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2020, 11, 22, 13, 57, 11, 60570),
 'log_count/DEBUG': 2,
 'log_count/ERROR': 1,
 'log_count/INFO': 12,
 'memusage/max': 57253888,
 'memusage/startup': 54648832,
 'response_received_count': 1,
 'scheduler/dequeued/redis': 1,
 'scheduler/enqueued/redis': 1,
 'spider_exceptions/KeyError': 1,
 'start_time': datetime.datetime(2020, 11, 22, 13, 55, 43, 329011)}
2020-11-22 21:57:11 [scrapy.core.engine] INFO: Spider closed (shutdown)
2020-11-22 21:57:13 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: dangdang)
2020-11-22 21:57:13 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'dangdang', 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'LOG_ENABLED': False, 'LOG_FILE': 'logging.log', 'NEWSPIDER_MODULE': 'dangdang.spiders', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['dangdang.spiders'], 'USER_AGENT': 'dangdang (+http://www.yourdomain.com)'}
2020-11-22 21:57:13 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2020-11-22 21:57:13 [book] INFO: Reading start URLs from redis key 'book:start_urls' (batch size: 16, encoding: utf-8
2020-11-22 21:57:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['dangdang.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'dangdang.middlewares.DangdangDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-11-22 21:57:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'dangdang.middlewares.DangdangSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-11-22 21:57:13 [scrapy.middleware] INFO: Enabled item pipelines:
['dangdang.pipelines.DangdangPipeline']
2020-11-22 21:57:13 [scrapy.core.engine] INFO: Spider opened
2020-11-22 21:57:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-11-22 21:57:13 [book] INFO: Spider opened: book
2020-11-22 21:57:13 [book] INFO: Spider opened: book
2020-11-22 21:57:18 [book] DEBUG: Read 1 requests from 'book:start_urls'
2020-11-22 21:57:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/23417701.html> (referer: https://www.dangdang.com)
2020-11-22 21:57:18 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.dangdang.com/23417701.html> (referer: https://www.dangdang.com)
Traceback (most recent call last):
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Users/nosun/data/python/scrapy/dangdang/dangdang/middlewares.py", line 39, in process_spider_output
    for i in result:
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/nosun/data/python/scrapy/dangdang/dangdang/spiders/book.py", line 32, in parse
    'category'] = self.get_book_detail(response)
  File "/Users/nosun/data/python/scrapy/dangdang/dangdang/spiders/book.py", line 109, in get_book_detail
    book_detail.css('li#detail-category-path>span.lie').xpath('a/text()').extract())
  File "/Users/nosun/data/python/scrapy/dangdang/dangdang/spiders/book.py", line 144, in format_book_detail
    arr = data.split('：')
AttributeError: 'list' object has no attribute 'split'
2020-11-22 21:58:13 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2020-11-22 21:58:50 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2020-11-22 21:58:50 [scrapy.core.engine] INFO: Closing spider (shutdown)
2020-11-22 21:58:50 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 720,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 30072,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2020, 11, 22, 13, 58, 50, 672831),
 'log_count/DEBUG': 2,
 'log_count/ERROR': 1,
 'log_count/INFO': 12,
 'memusage/max': 57479168,
 'memusage/startup': 54710272,
 'response_received_count': 1,
 'scheduler/dequeued/redis': 1,
 'scheduler/enqueued/redis': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2020, 11, 22, 13, 57, 13, 375632)}
2020-11-22 21:58:50 [scrapy.core.engine] INFO: Spider closed (shutdown)
2020-11-22 21:58:52 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: dangdang)
2020-11-22 21:58:52 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'dangdang', 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'LOG_ENABLED': False, 'LOG_FILE': 'logging.log', 'NEWSPIDER_MODULE': 'dangdang.spiders', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['dangdang.spiders'], 'USER_AGENT': 'dangdang (+http://www.yourdomain.com)'}
2020-11-22 21:58:52 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2020-11-22 21:58:52 [book] INFO: Reading start URLs from redis key 'book:start_urls' (batch size: 16, encoding: utf-8
2020-11-22 21:58:53 [scrapy.middleware] INFO: Enabled downloader middlewares:
['dangdang.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'dangdang.middlewares.DangdangDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-11-22 21:58:53 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'dangdang.middlewares.DangdangSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-11-22 21:58:53 [scrapy.middleware] INFO: Enabled item pipelines:
['dangdang.pipelines.DangdangPipeline']
2020-11-22 21:58:53 [scrapy.core.engine] INFO: Spider opened
2020-11-22 21:58:53 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-11-22 21:58:53 [book] INFO: Spider opened: book
2020-11-22 21:58:53 [book] INFO: Spider opened: book
2020-11-22 21:58:58 [book] DEBUG: Read 1 requests from 'book:start_urls'
2020-11-22 21:58:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/23417701.html> (referer: https://www.dangdang.com)
2020-11-22 21:58:58 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.dangdang.com/23417701.html> (referer: https://www.dangdang.com)
Traceback (most recent call last):
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Users/nosun/data/python/scrapy/dangdang/dangdang/middlewares.py", line 39, in process_spider_output
    for i in result:
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/nosun/data/python/scrapy/dangdang/dangdang/spiders/book.py", line 34, in parse
    book['category_path'] = self.get_category_path(response)
  File "/Users/nosun/data/python/scrapy/dangdang/dangdang/spiders/book.py", line 98, in get_category_path
    return categories[-1][31, -5]
TypeError: string indices must be integers
2020-11-22 21:59:49 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2020-11-22 21:59:49 [scrapy.core.engine] INFO: Closing spider (shutdown)
2020-11-22 21:59:49 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 623,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 30072,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2020, 11, 22, 13, 59, 49, 191197),
 'log_count/DEBUG': 2,
 'log_count/ERROR': 1,
 'log_count/INFO': 11,
 'memusage/max': 54812672,
 'memusage/startup': 54812672,
 'response_received_count': 1,
 'scheduler/dequeued/redis': 1,
 'scheduler/enqueued/redis': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2020, 11, 22, 13, 58, 53, 56898)}
2020-11-22 21:59:49 [scrapy.core.engine] INFO: Spider closed (shutdown)
2020-11-22 21:59:51 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: dangdang)
2020-11-22 21:59:51 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'dangdang', 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'LOG_ENABLED': False, 'LOG_FILE': 'logging.log', 'NEWSPIDER_MODULE': 'dangdang.spiders', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['dangdang.spiders'], 'USER_AGENT': 'dangdang (+http://www.yourdomain.com)'}
2020-11-22 21:59:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2020-11-22 21:59:51 [book] INFO: Reading start URLs from redis key 'book:start_urls' (batch size: 16, encoding: utf-8
2020-11-22 21:59:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['dangdang.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'dangdang.middlewares.DangdangDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-11-22 21:59:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'dangdang.middlewares.DangdangSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-11-22 21:59:51 [scrapy.middleware] INFO: Enabled item pipelines:
['dangdang.pipelines.DangdangPipeline']
2020-11-22 21:59:51 [scrapy.core.engine] INFO: Spider opened
2020-11-22 21:59:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-11-22 21:59:51 [book] INFO: Spider opened: book
2020-11-22 21:59:51 [book] INFO: Spider opened: book
2020-11-22 21:59:56 [book] DEBUG: Read 1 requests from 'book:start_urls'
2020-11-22 21:59:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/23417701.html> (referer: https://www.dangdang.com)
2020-11-22 21:59:56 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.dangdang.com/23417701.html> (referer: https://www.dangdang.com)
Traceback (most recent call last):
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Users/nosun/data/python/scrapy/dangdang/dangdang/middlewares.py", line 39, in process_spider_output
    for i in result:
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/nosun/data/python/scrapy/dangdang/dangdang/spiders/book.py", line 39, in parse
    yield scrapy.Request(url, callback=self.parse_book_detail(response), meta={'book': book})
  File "/Users/nosun/anaconda/envs/scrapy/lib/python3.6/site-packages/scrapy/http/request/__init__.py", line 31, in __init__
    raise TypeError('callback must be a callable, got %s' % type(callback).__name__)
TypeError: callback must be a callable, got generator
2020-11-22 22:00:32 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2020-11-22 22:00:32 [scrapy.core.engine] INFO: Closing spider (shutdown)
2020-11-22 22:00:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 625,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 33429,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2020, 11, 22, 14, 0, 32, 748437),
 'log_count/DEBUG': 2,
 'log_count/ERROR': 1,
 'log_count/INFO': 11,
 'memusage/max': 54792192,
 'memusage/startup': 54792192,
 'response_received_count': 1,
 'scheduler/dequeued/redis': 1,
 'scheduler/enqueued/redis': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2020, 11, 22, 13, 59, 51, 266674)}
2020-11-22 22:00:32 [scrapy.core.engine] INFO: Spider closed (shutdown)
2020-11-22 22:01:39 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: dangdang)
2020-11-22 22:01:39 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'dangdang', 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'LOG_ENABLED': False, 'LOG_FILE': 'logging.log', 'NEWSPIDER_MODULE': 'dangdang.spiders', 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['dangdang.spiders'], 'USER_AGENT': 'dangdang (+http://www.yourdomain.com)'}
2020-11-22 22:01:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2020-11-22 22:01:39 [book] INFO: Reading start URLs from redis key 'book:start_urls' (batch size: 16, encoding: utf-8
2020-11-22 22:01:39 [scrapy.middleware] INFO: Enabled downloader middlewares:
['dangdang.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'dangdang.middlewares.DangdangDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-11-22 22:01:39 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'dangdang.middlewares.DangdangSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-11-22 22:01:39 [scrapy.middleware] INFO: Enabled item pipelines:
['dangdang.pipelines.DangdangPipeline']
2020-11-22 22:01:39 [scrapy.core.engine] INFO: Spider opened
2020-11-22 22:01:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-11-22 22:01:39 [book] INFO: Spider opened: book
2020-11-22 22:01:39 [book] INFO: Spider opened: book
2020-11-22 22:01:44 [book] DEBUG: Read 1 requests from 'book:start_urls'
2020-11-22 22:01:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/23417701.html> (referer: https://www.dangdang.com)
2020-11-22 22:01:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/index.php?r=callback%2Fdetail&productId=23417701&templateType=publish&describeMap=&shopId=0&categoryPath=01.41.44.01.00.00> (referer: http://product.dangdang.com/23417701.html)
2020-11-22 22:01:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/index.php?r=comment%2Flist&productId=23417701&mainProductId=23417701&categoryPath=01.41.44.01.00.00&mediumId=0&pageIndex=1&sortType=1&filterType=1&isSystem=1&tagId=0&tagFilterCount=0&template=publish> (referer: http://product.dangdang.com/index.php?r=callback%2Fdetail&productId=23417701&templateType=publish&describeMap=&shopId=0&categoryPath=01.41.44.01.00.00)
2020-11-22 22:01:47 [scrapy.core.scraper] DEBUG: Scraped from <200 http://product.dangdang.com/index.php?r=comment%2Flist&productId=23417701&mainProductId=23417701&categoryPath=01.41.44.01.00.00&mediumId=0&pageIndex=1&sortType=1&filterType=1&isSystem=1&tagId=0&tagFilterCount=0&template=publish>
{'_id': '23417701',
 'author': '[日]吉田纯子 绘；徐超 译；心喜阅童书 出品',
 'author_detail': '',
 'catalog': '<p>《0～4岁幼儿认知小百科. 1》 '
            '适合刚刚开始学习说话和认识周围事物的孩子，内容涵盖动物、水果、蔬菜、交通工具、汽车、昆虫、植物、身体、颜色、数字等十个品类。</p><p>\n'
            '</p><p>《0～4岁幼儿认知小百科. 2 '
            '》以场景来分类，将孩子们带到街道、商店、餐厅、超市、医院、车站、海边、节日、幼儿园等地点，教孩子们认识这些特殊地点的专有事物。</p><p>\n'
            '</p><p>《0～4岁幼儿认知小百科. 3》 '
            '这套书中的进阶篇，带给那些想认识更多东西的孩子们。认知的事物包括食物、家庭、天空、鸟、玩具、乐器、服装、家居用品、城市设施。</p>',
 'category': ['图书',
              '童书',
              '婴儿读物',
              '认知书',
              '图书',
              '童书',
              '0-2岁',
              '认知',
              '图书',
              '童书',
              '双语/英语绘本'],
 'category_path': '01.41.44.01.00.00',
 'comments_info': {'autoCount': '377920',
                   'average_score': '5',
                   'average_score_eliminate_default': '4.7',
                   'favorable_rate': '1',
                   'goodRate': '99.9',
                   'long_comment_count': '1',
                   'main_product_id': '23417701',
                   'pageCount': '3693',
                   'pageIndex': '1',
                   'total_all_comment_num': '414848',
                   'total_auto_count': '377920',
                   'total_comment_num': '414847',
                   'total_crazy_count': '414409',
                   'total_detest_count': '135',
                   'total_image_count': '844',
                   'total_indifferent_count': '301',
                   'total_score_count': '36137'},
 'content': '<p>这套0-4岁幼儿认知小百科，全套三册，是日本低幼认知类图书中的明星产品。优点多到数不清呢，认知内容涵盖小朋友们在生活中几乎全部会碰到和接触到的东西，按照物品属性或者场景不同来分类，并且从易到难进阶排序。图书采用生动有趣的卡通图鉴形式，并附有中英双语，还可利用标签管理法快速查找。下面让我们来分别看一看这个系列的三本书吧。</p><p>系列中的*本适合刚刚开始学习说话和认识周围事物的孩子，内容涵盖动物、水果、蔬菜、交通工具、汽车、昆虫、植物、身体、颜色、数字等十个品类，每个品类都有侧边的标签标注出来，这一册所认识的事物以单个的形式出现。</p><p>系列中的第二本以场景来分类，将孩子们带到街道、商店、餐厅、超市、医院、车站、海边、节日、幼儿园等地点，教孩子们认识这些特殊地点的专有事物。</p><p>系列中的第三本是这套书中的进阶篇，带给那些想认识更多东西的孩子们。认知的事物包括食物、家庭、天空、鸟、玩具、乐器、服装、家居用品、城市设施。</p><p>收听音频：http://m.ximalaya.com/ertong/15953492/</p><p>这套0-4岁幼儿认知小百科，全套三册，是日本低幼认知类图书中的明星产品。优点多到数不清呢，认知内容涵盖小朋友们在生活中几乎全部会碰到和接触到的东西，按照物品属性或者场景不同来分类，并且从易到难进阶排序。图书采用生动有趣的卡通图鉴形式，并附有中英双语，还可利用标签管理法快速查找。下面让我们来分别看一看这个系列的三本书吧。</p><p>\n'
            '</p><p>系列中的*本适合刚刚开始学习说话和认识周围事物的孩子，内容涵盖动物、水果、蔬菜、交通工具、汽车、昆虫、植物、身体、颜色、数字等十个品类，每个品类都有侧边的标签标注出来，这一册所认识的事物以单个的形式出现。</p><p>\n'
            '</p><p>系列中的第二本以场景来分类，将孩子们带到街道、商店、餐厅、超市、医院、</p><p>车站、海边、节日、幼儿园等地点，教孩子们认识这些特殊地点的专有事物。</p><p>\n'
            '</p><p>系列中的第三本是这套书中的进阶篇，带给那些想认识更多东西的孩子们。认知的事物包括食物、家庭、天空、鸟、玩具、乐器、服装、家居用品、城市设施。</p><p>\n'
            '</p><p>收听音频</p><p>：http://m.ximalaya.com/ertong/15953492/</p><p>\n'
            '</p><p>\n'
            '</p><p>\n'
            '</p><p> \xa0</p><p>\n'
            '</p><p>\n'
            '</p><p>\n'
            '</p><p>\n'
            '</p><p>\n'
            '</p><p>\n'
            '</p><p>显示全部信息</p>',
 'description': '中英双语，升级音频。600多种认知物品，标签栏管理配超萌简笔画插图！(绘本0-3岁)(心喜阅童书出品)',
 'editor_recommendation': '<p>\n'
                          '</p><p>这是一套专门给0-4岁的孩子认知身边事物的小百科。这套书在原出版国日本可是幼儿认知书中的小明星――销售突破100万册，反复重印达35次</p><p>\n'
                          '</p><p>那么，如何让孩子更高效的使用这套书呢？</p><p>\n'
                          '</p><p>首先，观察封面。每本书的封面都标有这本书中所收录的9至10个认知类别，给孩子读一读，问问孩子想先读哪一种？</p><p>\n'
                          '</p><p>第二，利用这套书独有的标签栏。将孩子选中想要先读的那个部分找出来，标签栏有插图和文字两个部分，孩子也可以通过插图自己把喜欢的类别筛选出来。</p><p>\n'
                          '</p><p>第三，幼儿简笔画风格的插画，可以让孩子在认知之余，照书学画。</p><p>\n'
                          '</p><p>第四，英文部分不要忽略，这套小百科也可以当做字典来查阅哦！</p><p>\n'
                          '</p><p>0-4岁幼儿认知小百科·进阶版（全5册）</p><p>\xa0</p><p>\n'
                          '</p><p>\xa0</p>',
 'images': ['http://img3m1.ddimg.cn/43/31/23417701-1_u_7.jpg',
            'http://img3m1.ddimg.cn/43/31/23417701-2_u_4.jpg',
            'http://img3m1.ddimg.cn/43/31/23417701-3_u_4.jpg',
            'http://img3m1.ddimg.cn/43/31/23417701-4_u_4.jpg',
            'http://img3m1.ddimg.cn/43/31/23417701-5_u_3.jpg',
            'http://img3m1.ddimg.cn/43/31/23417701-6_u_3.jpg',
            'http://img3m1.ddimg.cn/43/31/23417701-7_u_3.jpg',
            'http://img3m1.ddimg.cn/43/31/23417701-8_u_3.jpg'],
 'isbn': '23417701',
 'media_review': '',
 'name': '0-4岁幼儿认知小百科（全3册)',
 'package': '平装-胶订',
 'paper': '胶版纸',
 'press': '长江少年儿童出版社',
 'price': 0,
 'pub_date': '2014年02月',
 'sell_price': '93.10',
 'size': '24开',
 'suit': '否',
 'url': 'http://product.dangdang.com/23417701.html'}
2020-11-22 22:02:39 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 3 pages/min), scraped 1 items (at 1 items/min)
2020-11-22 22:03:39 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 22:04:39 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 22:05:39 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 22:06:39 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 22:07:39 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 22:08:39 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 22:09:39 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 22:10:31 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2020-11-22 22:10:31 [scrapy.core.engine] INFO: Closing spider (shutdown)
2020-11-22 22:10:31 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2355,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 75502,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2020, 11, 22, 14, 10, 31, 212867),
 'item_scraped_count': 1,
 'log_count/DEBUG': 5,
 'log_count/INFO': 19,
 'memusage/max': 57856000,
 'memusage/startup': 54661120,
 'request_depth_max': 2,
 'response_received_count': 3,
 'scheduler/dequeued/redis': 3,
 'scheduler/enqueued/redis': 3,
 'start_time': datetime.datetime(2020, 11, 22, 14, 1, 39, 370156)}
2020-11-22 22:10:31 [scrapy.core.engine] INFO: Spider closed (shutdown)
2020-11-22 22:10:48 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: dangdang)
2020-11-22 22:10:48 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'dangdang', 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0, 'LOG_ENABLED': False, 'LOG_FILE': 'logging.log', 'NEWSPIDER_MODULE': 'dangdang.spiders', 'SPIDER_MODULES': ['dangdang.spiders'], 'USER_AGENT': 'dangdang (+http://www.yourdomain.com)'}
2020-11-22 22:10:48 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage']
2020-11-22 22:10:48 [scrapy.middleware] INFO: Enabled downloader middlewares:
['dangdang.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'dangdang.middlewares.DangdangDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-11-22 22:10:48 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'dangdang.middlewares.DangdangSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-11-22 22:10:48 [scrapy.middleware] INFO: Enabled item pipelines:
['dangdang.pipelines.DangdangPipeline']
2020-11-22 22:10:57 [scrapy.core.engine] INFO: Spider opened
2020-11-22 22:10:57 [default] INFO: Spider opened: default
2020-11-22 22:10:57 [default] INFO: Spider opened: default
2020-11-22 22:10:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/23417701.html> (referer: https://www.dangdang.com)
2020-11-22 22:15:42 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: dangdang)
2020-11-22 22:15:42 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'dangdang', 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_ENABLED': False, 'LOG_FILE': 'logging.log', 'NEWSPIDER_MODULE': 'dangdang.spiders', 'SPIDER_MODULES': ['dangdang.spiders'], 'USER_AGENT': 'dangdang (+http://www.yourdomain.com)'}
2020-11-22 22:15:42 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2020-11-22 22:15:42 [book] INFO: Reading start URLs from redis key 'book:start_urls' (batch size: 16, encoding: utf-8
2020-11-22 22:15:42 [scrapy.middleware] INFO: Enabled downloader middlewares:
['dangdang.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'dangdang.middlewares.DangdangDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-11-22 22:15:42 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'dangdang.middlewares.DangdangSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-11-22 22:15:42 [scrapy.middleware] INFO: Enabled item pipelines:
['dangdang.pipelines.DangdangPipeline']
2020-11-22 22:15:42 [scrapy.core.engine] INFO: Spider opened
2020-11-22 22:15:42 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-11-22 22:15:42 [book] INFO: Spider opened: book
2020-11-22 22:15:42 [book] INFO: Spider opened: book
2020-11-22 22:16:07 [book] DEBUG: Read 1 requests from 'book:start_urls'
2020-11-22 22:16:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/29116389.html> (referer: https://www.dangdang.com)
2020-11-22 22:16:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/index.php?r=callback%2Fdetail&productId=29116389&templateType=publish&describeMap=&shopId=0&categoryPath=01.41.05.03.00.00> (referer: http://product.dangdang.com/29116389.html)
2020-11-22 22:16:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/index.php?r=comment%2Flist&productId=29116389&mainProductId=29116389&categoryPath=01.41.05.03.00.00&mediumId=0&pageIndex=1&sortType=1&filterType=1&isSystem=1&tagId=0&tagFilterCount=0&template=publish> (referer: http://product.dangdang.com/index.php?r=callback%2Fdetail&productId=29116389&templateType=publish&describeMap=&shopId=0&categoryPath=01.41.05.03.00.00)
2020-11-22 22:16:10 [scrapy.core.scraper] DEBUG: Scraped from <200 http://product.dangdang.com/index.php?r=comment%2Flist&productId=29116389&mainProductId=29116389&categoryPath=01.41.05.03.00.00&mediumId=0&pageIndex=1&sortType=1&filterType=1&isSystem=1&tagId=0&tagFilterCount=0&template=publish>
{'_id': '29116389',
 'author': '[日]五味太郎、[日]柳生弦一郎、[日]佐藤雅彦 等著',
 'author_detail': '<p>《科学之友》50多年来培养了许多著名的科普童书作家，有稳定、成熟、强大的作者团队。此次引进出版的作者有读者人气王五味太郎、创意大师柳生弦一郎、知名设计师佐藤雅彦等。</p><p>[日]五味太郎\xa0'
                  '</p><p>1945 '
                  '年出生，日本桑泽设计研究所工业设计系毕业，后专职创作童书，成为日本畅销童书作家之一。已出版400 '
                  '多本图画书，曾获产经儿童出版文化奖，博洛尼亚国际绘本原画展奖等多项奖项，代表作有《鳄鱼怕怕\u3000'
                  '牙医怕怕》《小牛的春天》等。五味太郎的作品和孩子们的生活、成长息息相关，不说教且蕴含智慧，语言富有幽默感，绘画稚拙有趣，每本书都有绝妙的创意，高产但从不自我重复，常常给孩子们带来无穷的惊喜。</p><p>[日]柳生弦一郎</p><p>1943 '
                  '年出生于日本三重县，东京时尚插画学院毕业，后专职创作童书，成为知名童书作家，创作了很多以身体为主题的科普图画书。1967 '
                  '年荣获讲谈社出版文化奖插画奖，作品有《乳房的秘密》《脚丫子的故事》等。作品风格夸张幽默，稚拙可爱，能站在孩子的视角讲述科学知识，总能逗得孩子们哈哈大笑。</p>',
 'catalog': '',
 'category': '所属分类：图书>童书>科普/百科>科普图书>童书>3-6岁>科普/百科图书>童书>科学认知',
 'category_path': '01.41.05.03.00.00',
 'comments_info': {'autoCount': '1098',
                   'average_score': '4.98',
                   'average_score_eliminate_default': '4.87',
                   'favorable_rate': '1',
                   'goodRate': '99.6',
                   'long_comment_count': '0',
                   'main_product_id': '29116389',
                   'pageCount': '5',
                   'pageIndex': '1',
                   'total_auto_count': '1098',
                   'total_comment_num': '1144',
                   'total_crazy_count': '1139',
                   'total_detest_count': '1',
                   'total_image_count': '12',
                   'total_indifferent_count': '0',
                   'total_score_count': '42'},
 'content': '<p>《科学之友》是“日本绘本之父”松居直策划的，世界上*个科学绘本月刊，它秉承着“培养孩子科学之心”的出版宗旨，自 1969 '
            '年创办至今，出版了 600 多本科普书，是畅销日本 50 年的经典科普品牌。</p><p>\u3000\u3000'
            '本次精选其中的12册出版，向孩子们展现丰富多彩的科学世界。这套书真正立足于儿童视角，用细腻生动的描绘唤醒孩子的五感，激发他们对世界的好奇心和探索欲，培养科学素养必备的观察力、想象力、创造力、思考能力。这套书绘画风格多样，有清新写实，有稚拙幽默，呈现世界的惊奇和美感。</p><p>\u3000\u3000'
            '书中有想象小水滴聚成大海的思维挑战，有用一张纸折出动物园的动手训练，</p><p>有用 X '
            '光照一照椅子内部结构的创想实验，有对植物生长的细致观察，有让孩子练习从小做决定的创意难题……让孩子们体验观察、认知、思考、探索、想象的乐趣，迈入不可思议的科学大门。</p><p>单册介绍：</p><p>《小水滴聚起来》 '
            '思想实验、科学思维、质和量、想象力</p><p>\xa0 \xa0 '
            '《科学之友》是“日本绘本之父”松居直策划的，世界上*个科学绘本月刊，它秉承着“培养孩子科学之心”的出版宗旨，自 1969 '
            '年创办至今，出版了 600 多本科普书，是畅销日本 50 年的经典科普品牌。</p><p>\u3000\u3000'
            '本次精选其中的12册出版，向孩子们展现丰富多彩的科学世界。这套书真正立足于儿童视角，用细腻生动的描绘唤醒孩子的五感，激发他们对世界的好奇心和探索欲，培养科学素养必备的观察力、想象力、创造力、思考能力。这套书绘画风格多样，有清新写实，有稚拙幽默，呈现世界的惊奇和美感。</p><p>\u3000\u3000'
            '书中有想象小水滴聚成大海的思维挑战，有用一张纸折出动物园的动手训练，</p><p>有用 X '
            '光照一照椅子内部结构的创想实验，有对植物生长的细致观察，有让孩子练习从小做决定的创意难题……让孩子们体验观察、认知、思考、探索、想象的乐趣，迈入不可思议的科学大门。</p><p>单册介绍：</p><p>《小水滴聚起来》 '
            '思想实验、科学思维、质和量、想象力</p><p>一滴一滴小雨滴聚在一起，变成一匙水；好多匙水聚在一起，变成一杯水；水越聚越多，变成湖，变成汪洋大海……大海的水和一滴雨珠有什么相同？又有什么不同？</p><p>孩子总会思考地球究竟有多大、全世界有多少水这样寻根问底的大问题，其实可以从一个小雨滴中找到答案。本书设计了一个有幻想色彩的思维游戏，帮助孩子理解质和量的变化关系，以小见大，在一滴水中看见世界。</p><p>\xa0'
            '</p><p>《恐龙究竟有多大》 '
            '远古生物、亲近生活、数学测量、空间思维</p><p>地球上生活过大大小小的恐龙，让我们将恐龙与身边熟悉的东西比一比。三角龙和公园的攀爬架差不多大，暴龙有路灯那么高，和巴士一样大；如果地震龙来到动物园，和陆地上*的动物非洲象比比个头，谁大？……</p><p>本书从“大小”的角度介绍各种恐龙，将恐龙带到日常生活中，让孩子们亲近远古的神秘恐龙，想和恐龙一起玩耍。</p><p>\n'
            '</p><p>《外面和里面》 想象力、空间思维、美育、多角度认知 \xa0'
            '</p><p>当你看一样东西时，你想知道它的里面是什么样吗？用X光照一照，椅子、储蓄罐和圆珠笔，你能看到平常看不见的东西，神奇极了！请尽情享受，想象物体内部结构的乐趣。</p><p>一本简单、有趣、有设计美感的图画书，让孩子们明白从多种视角看待事物的重要性，激发好奇心，培养孩子的想象力和思维能力。</p><p>\n'
            '</p><p>《两个我》 自我意识、情绪认知、幽默趣味</p><p>\xa0'
            '“我”的身体里有两个我：一个我，想按时起床、想吃早饭、想做家务、有梦想；另一个我，想赖床、想吃零食、想偷懒、没自信……“我”的心里总有各种各样的念头，我的想法总在改变，同一件事我怎么想、大家怎么想？这不是深奥的哲学问题，是每一个孩子形成自我意识的必经之路。</p><p>\n'
            '</p><p>《如果是你怎么办》思想实验、儿童心理、幽默趣味</p><p>要登上山顶，有很多种方法：走路、开汽车、坐巴士、搭缆车、乘直升机，你选择怎样去？有奇奇怪怪的面包，你要不要吃？小狗掉水里，你要救它吗？……在一个接一个问题面前，你会选择怎么办？ \xa0'
            '本书有14个有趣的创意难题，让孩子们训练自己做决定。但是，书里没有标准答案哦，因为人生也没有正确的答案。选择很困难，也很有趣！我们都要认真地做出选择。</p><p>\n'
            '</p><p>《小肚子咕咕叫》 \xa0'
            '幽默趣味、消化的科学机制、生活习惯</p><p>什么时候，我们会肚子咕咕叫？当体内的能量消耗完，我们就会感到饥饿。吃是很重要、很快乐的事情！可是，饭菜很美味，零食更诱惑……</p><p>这是一本让孩子们胃口大开的图画书，有“创意鬼才”之称的柳生弦一郎用个性十足的画风、幽默夸张的文字讲解孩子贪吃的心理和食物维持生命活动的科学机制。本书*能让孩子们哈哈大笑，同时养成好好吃饭的习惯。</p><p>\n'
            '</p><p>《蚂蚁的家》 \xa0'
            '观察能力、探索自然、生命教育</p><p>蚂蚁一家在公园的地底下生活，蚁后在巢里产卵，一只只小工蚁出生后，它们齐心协力地生活、相互支持、扩大家族。但是有一天，巢穴被石头包围住了，怎么办？……</p><p>孩子们都见过墙角石缝间匆匆爬过的蚂蚁，好奇它们怎样生活。本书激发孩子的探索欲，真实直观地揭开蚂蚁一家的秘密生活，记录蚂蚁的生命历程。原来蚂蚁和人类一样，对家族成员怀有强烈的爱。每个小生命的生存都不容易，小蚂蚁触动我们柔软的心，要尊重生命，热爱自然啊！</p><p>《玩具医院》 \xa0 '
            '玩具修理、动手能力、爱惜物品 \xa0\xa0</p><p>显示全部信息</p>',
 'description': '“日本绘本之父”松居直策划，畅销50多年经典科普品牌，五味太郎等名家联袂创作，3—6岁适读。彭懿翻译，阿甲、王林、王志庚、朱自强、周其星、姚颖推荐，大J小D、三川玲、钱儿爸、斯坦福妈妈、张丹丹等盛赞。',
 'editor_recommendation': '<p>★“日本绘本之父”松居直策划，畅销50年世界经典科普图画书品牌</p><p>★儿童文学作家、翻译家彭懿倾情翻译，阿甲、王林、王志庚、朱自强、周其星、姚颖联袂推荐</p><p>★大J小D、童书妈妈三川玲、Michael钱儿频道、大象公会、果壳童书馆、小十点、斯坦福妈妈、张丹丹等多位育儿大V盛赞，上市一个月销售10万册</p><p>★诺奖得主山中伸弥、生物学家福冈伸一推荐阅读</p><p>★3—6岁孩子适读，有故事性、有温度、有创意的科普图画书，涵盖动物、植物、实验、身体、生活现象等多个主题，让孩子对科学产生持久的兴趣</p><p>★不以灌输知识为目的，注重激发好奇心和探索欲，培养科学素养必备的观察力、想象力、创造力、思考能力</p><p>★五味太郎、柳生弦一郎、佐藤雅彦等绘本名家联手打造</p><p>\n'
                          '</p><p>内容特点：</p><p>★“科学之友经典图画书”是一套培养国民科学素养的图画书。很多教育专家认为：日本近几十年来教育的成功完全归功于图画书。《科学之友》是“日本绘本之父”松居直投入大量心血创办的刊物，也是世界上*部专门为孩子打造的科学图画书月刊，富有开创意义。50多年来《科学之友》已陪伴了几代日本科学家成长，也一直在坚持着寻找吸引孩子们迈入科学大门的方法，这在全世界上都是*的。</p><p>★《科学之友》有一句slogan: '
                          '科学之友，与世界做朋友。这套书的目的不是灌输知识，而是要激发孩子们的好奇心，让他们体验“观察”“认知”“想象”“思考”“探索”的乐趣，从而对科学产生持久的兴趣，并把这种对科学的爱融入到日常生活当中：与科学为友，也与世界为友。</p><p>\xa0'
                          '</p>',
 'images': ['http://img3m9.ddimg.cn/93/16/29116389-1_u_12.jpg',
            'http://img3m9.ddimg.cn/93/16/29116389-2_u_6.jpg',
            'http://img3m9.ddimg.cn/93/16/29116389-3_u_6.jpg',
            'http://img3m9.ddimg.cn/93/16/29116389-4_u_5.jpg',
            'http://img3m9.ddimg.cn/93/16/29116389-5_u_6.jpg',
            'http://img3m9.ddimg.cn/93/16/29116389-6_u_6.jpg',
            'http://img3m9.ddimg.cn/93/16/29116389-7_u_6.jpg',
            'http://img3m9.ddimg.cn/93/16/29116389-8_u_6.jpg'],
 'isbn': '9787544866071',
 'media_review': '<p>《科学之友》让孩子在感到愉悦的情境中，以游戏的方式，鼓励他们自主观察、发现、体验，更能产生不可磨灭的印象，从而内化为他们自己的知识。而更重要的是，他们可以从中获得观察与实践的方法，不断应用于日常生活中，继续发现并拓展。也许可以这样说，*好的科普读本，带来的*收益应该是在书本之外的。</p><p>——阅读推广人阿甲</p><p> '
                 '——知名阅读教师、儿童阅读推广人、三叶草故事家族创始人  周其星</p><p>  '
                 '日本是有着先进科学教育理念、丰富科学教育实践的国家，有很多宝贵的经验可供借鉴。……运用生动有趣的绘本传达科学教育的意义和内容，形成幼儿基本的科学素养，是相较而言更加经济便捷、更加符合儿童审美接受的有益的方式。</p><p> '
                 '“科学之友”在科学主题和内容覆盖上非常全面。这套书拥有超强的创作阵容，吸引了包括五味太郎、佐藤雅彦在内的、一大批日本知名作家、画家、科学家、教育家共同参与，为孩子们悉心打造出一个个富有情趣的、细腻生动的好故事。 '
                 '“科学之友”巧妙结合了科学性、人文性和艺术性，帮助幼儿开启探究外部世界和认识内部世界的眼睛，同时也为孩子们插上一对想象的翅膀，在科学与幻想间自由驰骋，在文学和艺术中尽情欢愉。值得亲子共读，也适合在幼儿园和学校科学活动中广泛运用。</p><p>《科学之友》让孩子在感到愉悦的情境中，以游戏的方式，鼓励他们自主观察、发现、体验，更能产生不可磨灭的印象，从而内化为他们自己的知识。而更重要的是，他们可以从中获得观察与实践的方法，不断应用于日常生活中，继续发现并拓展。也许可以这样说，*好的科普读本，带来的*收益应该是在书本之外的。</p><p>——阅读推广人阿甲</p><p>\n'
                 '</p><p>对孩子进行科学教育，需要生活的味道，需要儿童的趣味，需用从孩子的视角去打量去好奇去追问去探寻，而不是从知识的角度去灌输去讲解。这一点，来自日本的这套《科学之友》显然做得特别好，以至于让我这样一个大人，也看得津津有味，不只是因为懂得了生活中的常识，更是被其中天真的语气所吸引，也被这群可爱的作者所打动。儿童的身体是柔软的，出现在儿童世界里的一切事物也应当是柔和的。生硬的教育请离开。好玩的故事请进来。</p><p>\xa0'
                 '——知名阅读教师、儿童阅读推广人、三叶草故事家族创始人 \xa0周其星</p><p>\n'
                 '</p><p>\xa0 '
                 '日本是有着先进科学教育理念、丰富科学教育实践的国家，有很多宝贵的经验可供借鉴。……运用生动有趣的绘本传达科学教育的意义和内容，形成幼儿基本的科学素养，是相较而言更加经济便捷、更加符合儿童审美接受的有益的方式。</p><p>\xa0'
                 '“科学之友”在科学主题和内容覆盖上非常全面。这套书拥有超强的创作阵容，吸引了包括五味太郎、佐藤雅彦在内的、一大批日本知名作家、画家、科学家、教育家共同参与，为孩子们悉心打造出一个个富有情趣的、细腻生动的好故事。 '
                 '“科学之友”巧妙结合了科学性、人文性和艺术性，帮助幼儿开启探究外部世界和认识内部世界的眼睛，同时也为孩子们插上一对想象的翅膀，在科学与幻想间自由驰骋，在文学和艺术中尽情欢愉。值得亲子共读，也适合在幼儿园和学校科学活动中广泛运用。</p><p>\xa0 '
                 '——北京师范大学副教授 \xa0姚颖</p><p>\n'
                 '</p><p>\xa0 \xa0 '
                 '《科学之友》让孩子在阅读绘本之后，对平日司空见惯的事物也能有新发现。这个品牌迎来了创刊50周年纪念，即使在全世界上，也是*的。即使时代和环境在变化，他们也一直在坚持，寻找能够吸引孩子们迈入科学大门的方法。</p><p>\xa0 '
                 '——日本 朝日新闻</p><p>\n'
                 '</p><p>\xa0 \xa0 '
                 '给孩子们介绍科学的趣味性绘本《科学之友》在2019年迎来了创刊50周年。*初创刊以孩子为阅读对象，向他们传达这个到处充满着“不可思议”的世界为宗旨。内容主要身边的事物为中心，经过精心取材，已经有许多优秀的作品面世。</p><p>\xa0 \xa0 '
                 '在《科学之友》创刊前，日本还没有科学绘本这个分类。大多是图鉴，或者类似教科书的书，于是编辑们探讨了“不是为了给孩子灌输知识，而是创作趣味性强的科学绘本”的方针和构想。</p><p>\xa0 \xa0 '
                 '在精心的取材和观察的基础上，还要对画稿反复修改。因此，每本书从创作到面世至少要经过3年，时间久的甚至达到8年。“为了让孩子感受科学的真正的趣味性，而且不仅是‘看’，还要让孩子进行更深层次的观察和启发性思考。”这句由*任主编写入创刊致辞里的理念，现在仍一直被践行着。</p><p>——日本 '
                 '每日新闻</p><p>\n'
                 '</p><p>\xa0</p><p>显示全部信息</p>',
 'name': '科学之友经典图画书（礼盒装共12册）',
 'package': '盒装',
 'paper': '铜版纸',
 'press': '接力出版社',
 'price': '300.00',
 'pub_date': '2020年08月',
 'sell_price': '300.00',
 'size': '12开',
 'suit': '是',
 'url': 'http://product.dangdang.com/29116389.html'}
2020-11-22 22:16:27 [book] DEBUG: Read 1 requests from 'book:start_urls'
2020-11-22 22:16:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/25293542.html#ddclick_reco_reco_alsoview> (referer: https://www.dangdang.com)
2020-11-22 22:16:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/index.php?r=callback%2Fdetail&productId=25293542&templateType=publish&describeMap=&shopId=0&categoryPath=01.41.48.07.00.00> (referer: http://product.dangdang.com/25293542.html)
2020-11-22 22:16:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/index.php?r=comment%2Flist&productId=25293542&mainProductId=25293542&categoryPath=01.41.48.07.00.00&mediumId=0&pageIndex=1&sortType=1&filterType=1&isSystem=1&tagId=0&tagFilterCount=0&template=publish> (referer: http://product.dangdang.com/index.php?r=callback%2Fdetail&productId=25293542&templateType=publish&describeMap=&shopId=0&categoryPath=01.41.48.07.00.00)
2020-11-22 22:16:30 [scrapy.core.scraper] DEBUG: Scraped from <200 http://product.dangdang.com/index.php?r=comment%2Flist&productId=25293542&mainProductId=25293542&categoryPath=01.41.48.07.00.00&mediumId=0&pageIndex=1&sortType=1&filterType=1&isSystem=1&tagId=0&tagFilterCount=0&template=publish>
{'_id': '25293542',
 'author': '天才教育',
 'author_detail': '',
 'catalog': '',
 'category': '所属分类：图书>童书>玩具书>其他图书>童书>3-6岁>玩具书图书>童书>数学启蒙',
 'category_path': '01.41.48.07.00.00',
 'comments_info': {'autoCount': '35434',
                   'average_score': '4.87',
                   'average_score_eliminate_default': '4.56',
                   'favorable_rate': '1',
                   'goodRate': '99.9',
                   'long_comment_count': '1',
                   'main_product_id': '25293542',
                   'pageCount': '82',
                   'pageIndex': '1',
                   'total_all_comment_num': '36254',
                   'total_auto_count': '35434',
                   'total_comment_num': '36253',
                   'total_crazy_count': '36216',
                   'total_detest_count': '24',
                   'total_image_count': '242',
                   'total_indifferent_count': '8',
                   'total_score_count': '775'},
 'content': '',
 'description': '数学一玩就会！好看、好懂、玩不厌的数学立体操作绘本，天才教育原版引进，3岁+数学启蒙，游戏中打造数学脑！（全套22册）',
 'editor_recommendation': '',
 'images': ['http://img3m2.ddimg.cn/32/9/25293542-1_u_24.jpg',
            'http://img3m2.ddimg.cn/32/9/25293542-2_u_12.jpg',
            'http://img3m2.ddimg.cn/32/9/25293542-3_u_7.jpg',
            'http://img3m2.ddimg.cn/32/9/25293542-4_u_5.jpg'],
 'isbn': '9787549971497',
 'media_review': '',
 'name': '神奇的数学（礼品装，包含11本纸板书和11本练习册）',
 'package': '精装',
 'paper': '胶版纸',
 'press': '江苏教育出版社',
 'price': '798.00',
 'pub_date': '2018年04月',
 'sell_price': '292.70',
 'size': '4开',
 'suit': '是',
 'url': 'http://product.dangdang.com/25293542.html#ddclick_reco_reco_alsoview'}
2020-11-22 22:16:42 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 6 pages/min), scraped 2 items (at 2 items/min)
2020-11-22 22:17:02 [book] DEBUG: Read 1 requests from 'book:start_urls'
2020-11-22 22:17:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/27868866.html> (referer: https://www.dangdang.com)
2020-11-22 22:17:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/index.php?r=callback%2Fdetail&productId=27868866&templateType=publish&describeMap=&shopId=0&categoryPath=01.41.44.13.00.00> (referer: http://product.dangdang.com/27868866.html)
2020-11-22 22:17:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/index.php?r=comment%2Flist&productId=27868866&mainProductId=27868866&categoryPath=01.41.44.13.00.00&mediumId=0&pageIndex=1&sortType=1&filterType=1&isSystem=1&tagId=0&tagFilterCount=0&template=publish> (referer: http://product.dangdang.com/index.php?r=callback%2Fdetail&productId=27868866&templateType=publish&describeMap=&shopId=0&categoryPath=01.41.44.13.00.00)
2020-11-22 22:17:04 [scrapy.core.scraper] DEBUG: Scraped from <200 http://product.dangdang.com/index.php?r=comment%2Flist&productId=27868866&mainProductId=27868866&categoryPath=01.41.44.13.00.00&mediumId=0&pageIndex=1&sortType=1&filterType=1&isSystem=1&tagId=0&tagFilterCount=0&template=publish>
{'_id': '27868866',
 'author': '[日]入山智 著/绘；崔维燕 译；心喜阅童书 出品',
 'author_detail': '<p>\xa0</p>',
 'catalog': '<p>★《小鸡球球洞洞认知书》\xa0 \xa0（全4册）</p><p>\n'
            '</p><p>★《小鸡球球触感玩具书》\xa0 \xa0（全5册）</p><p>\n'
            '</p><p>★《和小鸡球球一起玩》\xa0 \xa0 \xa0 （全6册）</p><p>\n'
            '</p><p>★《小鸡球球成长绘本系列》（全9册）</p><p>\n'
            '</p><p>★《小鸡球球生命友情系列》（全2册）</p><p>\n'
            '</p><p>★《小鸡球球变点心》</p><p>\n'
            '</p><p>★《小鸡球球的春夏秋冬》</p><p>\n'
            '</p><p>★《小鸡球球自然双语认知图鉴》</p>',
 'category': '所属分类：图书>童书>婴儿读物>纸板书图书>童书>0-2岁>纸板书图书>童书>双语/英语绘本',
 'category_path': '01.41.44.13.00.00',
 'comments_info': {'autoCount': '298471',
                   'average_score': '4.86',
                   'average_score_eliminate_default': '4.52',
                   'favorable_rate': '1',
                   'goodRate': '99.9',
                   'long_comment_count': '2',
                   'main_product_id': '27868866',
                   'pageCount': '932',
                   'pageIndex': '1',
                   'total_all_comment_num': '307785',
                   'total_auto_count': '298471',
                   'total_comment_num': '307783',
                   'total_crazy_count': '307444',
                   'total_detest_count': '160',
                   'total_image_count': '690',
                   'total_indifferent_count': '177',
                   'total_score_count': '9302'},
 'content': '<p>《小鸡球球大家族》共29册，当当网0~4岁婴幼儿读物领头产品，全方位满足0~4岁早期阅读需求。其中有给孩子0~2岁使用的洞洞认知书、触感玩具书、互动纸板书；1~3岁使用的立体阅读绘本、创意联想绘本；3岁 '
            '阅读的生命教育、自然教育绘本，以及自然双语认知大图鉴。随书附赠《从0岁开始阅读手册》，从近三十年世界多项早期阅读的研究出发，讲述“为什么要从0岁开始阅读”；按月龄说明孩子的发展特点，该读什么书，如何选择“小鸡球球”系列产品；视频演示如何跟小月龄宝宝互动阅读。</p><p>\n'
            '</p><p>\xa0</p><p>\n'
            '</p><p>\xa0\xa0\xa0\xa0'
            '暖萌、可爱、精美的包装，更是送给刚刚开始早期阅读或者喜欢小鸡球球的孩子，作为“阅读礼”的不二选择！</p>',
 'description': '当当网0~4岁婴儿读物领头产品，50万妈妈口碑推荐，万千孩子0岁阅读的起点！从洞洞认知书、触感玩具书、立体阅读绘本到创意互动绘本、双语图鉴，小鸡球球陪伴孩子早期发展每一步（海豚传媒出品）',
 'editor_recommendation': '<p>\n'
                          '</p><p>（1）畅销百万册“小鸡球球”首发29册大合辑，当当网0~4岁婴幼儿读物领头产品，陪伴孩子从0岁开始阅读，助力孩子成为“终生阅读者”。</p><p>\n'
                          '</p><p>（2）从给孩子0~2岁使用的洞洞认知书、触感玩具书、互动纸板书，到1~3岁使用的立体阅读绘本、创意联想绘本，再到3岁 '
                          '阅读的生命教育、自然教育绘本，以及双语认知大图鉴，29册大合辑，全方位满足0~4岁孩子各阶段的阅读需求。</p><p>（3）附赠《从0岁开始阅读》手册，从近三十年世界多项早期阅读的研究出发，讲述“为什么要从0岁开始阅读”；按月龄说明孩子的发展特点，该读什么书，如何选择“小鸡球球”系列产品；视频演示如何跟小月龄宝宝互动阅读。</p><p>（4）精美礼盒包装，很适合作为礼物送给刚刚开始早期阅读的孩子，以及喜欢小鸡球球的孩子。</p><p>\n'
                          '</p><p>（5）限量赠送日本原版引进小鸡球球限量版公仔，数量有限，赠完即止【</p><p>部分页面下单需自行勾选赠品，否则无法赠送，如有疑问请咨询客服</p><p>】</p><p>\n'
                          '</p><p>\xa0</p>',
 'images': ['http://img3m6.ddimg.cn/69/22/27868866-1_u_89.jpg',
            'http://img3m6.ddimg.cn/69/22/27868866-2_u_20.jpg',
            'http://img3m6.ddimg.cn/69/22/27868866-3_u_23.jpg',
            'http://img3m6.ddimg.cn/69/22/27868866-4_u_18.jpg'],
 'isbn': '9787553510156',
 'media_review': '',
 'name': '小鸡球球大家族（全8大系列）',
 'package': '盒装',
 'paper': '胶版纸',
 'press': '长江少年儿童出版社',
 'price': '978.80',
 'pub_date': '2019年05月',
 'sell_price': '880.90',
 'size': '48开',
 'suit': '是',
 'url': 'http://product.dangdang.com/27868866.html'}
2020-11-22 22:17:42 [scrapy.extensions.logstats] INFO: Crawled 9 pages (at 3 pages/min), scraped 3 items (at 1 items/min)
2020-11-22 22:18:42 [scrapy.extensions.logstats] INFO: Crawled 9 pages (at 0 pages/min), scraped 3 items (at 0 items/min)
2020-11-22 22:19:42 [scrapy.extensions.logstats] INFO: Crawled 9 pages (at 0 pages/min), scraped 3 items (at 0 items/min)
2020-11-22 22:20:42 [scrapy.extensions.logstats] INFO: Crawled 9 pages (at 0 pages/min), scraped 3 items (at 0 items/min)
2020-11-22 22:21:42 [scrapy.extensions.logstats] INFO: Crawled 9 pages (at 0 pages/min), scraped 3 items (at 0 items/min)
2020-11-22 22:22:07 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2020-11-22 22:22:07 [scrapy.core.engine] INFO: Closing spider (shutdown)
2020-11-22 22:22:07 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 7211,
 'downloader/request_count': 9,
 'downloader/request_method_count/GET': 9,
 'downloader/response_bytes': 223459,
 'downloader/response_count': 9,
 'downloader/response_status_count/200': 9,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2020, 11, 22, 14, 22, 7, 187763),
 'item_scraped_count': 3,
 'log_count/DEBUG': 15,
 'log_count/INFO': 17,
 'memusage/max': 60526592,
 'memusage/startup': 54681600,
 'request_depth_max': 2,
 'response_received_count': 9,
 'scheduler/dequeued': 9,
 'scheduler/dequeued/memory': 9,
 'scheduler/enqueued': 9,
 'scheduler/enqueued/memory': 9,
 'start_time': datetime.datetime(2020, 11, 22, 14, 15, 42, 445029)}
2020-11-22 22:22:07 [scrapy.core.engine] INFO: Spider closed (shutdown)
2020-11-22 22:22:10 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: dangdang)
2020-11-22 22:22:10 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'dangdang', 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_ENABLED': False, 'LOG_FILE': 'logging.log', 'NEWSPIDER_MODULE': 'dangdang.spiders', 'SPIDER_MODULES': ['dangdang.spiders'], 'USER_AGENT': 'dangdang (+http://www.yourdomain.com)'}
2020-11-22 22:22:10 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2020-11-22 22:22:10 [book] INFO: Reading start URLs from redis key 'book:start_urls' (batch size: 16, encoding: utf-8
2020-11-22 22:22:10 [scrapy.middleware] INFO: Enabled downloader middlewares:
['dangdang.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'dangdang.middlewares.DangdangDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-11-22 22:22:10 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'dangdang.middlewares.DangdangSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-11-22 22:22:10 [scrapy.middleware] INFO: Enabled item pipelines:
['dangdang.pipelines.DangdangPipeline']
2020-11-22 22:22:10 [scrapy.core.engine] INFO: Spider opened
2020-11-22 22:22:10 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-11-22 22:22:10 [book] INFO: Spider opened: book
2020-11-22 22:22:10 [book] INFO: Spider opened: book
2020-11-22 22:22:45 [book] DEBUG: Read 1 requests from 'book:start_urls'
2020-11-22 22:22:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/27874767.html> (referer: https://www.dangdang.com)
2020-11-22 22:22:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/index.php?r=callback%2Fdetail&productId=27874767&templateType=publish&describeMap=&shopId=0&categoryPath=01.41.48.02.00.00> (referer: http://product.dangdang.com/27874767.html)
2020-11-22 22:22:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://product.dangdang.com/index.php?r=comment%2Flist&productId=27874767&mainProductId=27874767&categoryPath=01.41.48.02.00.00&mediumId=0&pageIndex=1&sortType=1&filterType=1&isSystem=1&tagId=0&tagFilterCount=0&template=publish> (referer: http://product.dangdang.com/27874767.html)
2020-11-22 22:22:48 [scrapy.core.scraper] DEBUG: Scraped from <200 http://product.dangdang.com/index.php?r=comment%2Flist&productId=27874767&mainProductId=27874767&categoryPath=01.41.48.02.00.00&mediumId=0&pageIndex=1&sortType=1&filterType=1&isSystem=1&tagId=0&tagFilterCount=0&template=publish>
{'_id': '27874767',
 'author': '【英】DK公司 著',
 'author_detail': '<p>英国DK公司，成立于1974年，是国际知名的出版社，被誉为全球视觉工具书领导品牌。DK图书既有视觉冲击力又包含丰富的信息内容，被认为是从生到老不断学习的生活指南，其科普、百科、生活等图书畅销全球。</p><p>\xa0'
                  '</p>',
 'catalog': '',
 'category': '所属分类：图书>童书>玩具书>触摸/洞洞/手偶书图书>童书>3-6岁>玩具书',
 'category_path': '01.41.48.02.00.00',
 'comments_info': {'autoCount': '8515',
                   'average_score': '4.97',
                   'average_score_eliminate_default': '4.93',
                   'favorable_rate': '1',
                   'goodRate': '99.8',
                   'long_comment_count': '0',
                   'main_product_id': '27874767',
                   'pageCount': '49',
                   'pageIndex': '1',
                   'total_auto_count': '8515',
                   'total_comment_num': '8998',
                   'total_crazy_count': '8983',
                   'total_detest_count': '4',
                   'total_image_count': '70',
                   'total_indifferent_count': '11',
                   'total_score_count': '467'},
 'content': '<p>抠一抠，瞅一瞅，洞洞里面有什么？</p><p>\n'
            '</p><p>来自英国DK的互动玩具书，躲猫猫游戏、认知小百科，猜猜我是谁、视觉大发现，快乐探索、趣味模仿，双语学习、思维开发，好适合0～4岁处于敏感期的好奇宝宝！</p><p>\n'
            '</p><p>高清大图，真实情境，实物认知。根据文字提示、图片线索和局部细节，宝宝能猜到洞洞后面藏着谁吗？它们有什么标志性的动作和声音呢？快跟着学一学吧！在小惊喜和大发现中，宝宝的手眼协调能力、细节观察能力、思维想象能力和模仿能力一起提升！</p><p>\n'
            '</p><p>6大主题，40多个趣味洞洞、150多个英语词汇、300多条百科信息、500多个日常短句，高品质、大容量认知读物，亲子互动共读、启蒙独立阅读。</p><p>\n'
            '</p><p>附赠免费中英双语音频，听声猜物、有声伴读，畅听学习无负担，可读、可看、可听、可玩，满足多元需求。</p><p>\n'
            '</p><p>\xa0</p>',
 'description': '英国DK双语互动玩具书！抠洞洞，躲猫猫，猜猜我是谁？ '
                '高清真图、实物认知，好奇洞洞、百科知识，中英双语、免费音频，快乐探索、趣味模仿，思维开发、亲子互动！0-4岁',
 'editor_recommendation': '<p>1.DK品牌，品质保证，高清大图，实物认知。</p><p>来自英国DK的双语互动玩具书，国际品牌，专业水准。</p><p>精美、真实的图片，更利于宝宝对未知世界形成正确的认知。</p><p>2.宝宝酷爱的内容大集合。</p><p>萌宠来袭：《宠物小乖乖》《动物小可爱》； '
                          '车辆集结：《卡车，真能干》《拖拉机，帮帮忙》；动物出没：《农场伙伴玩游戏》《野生动物跑出来》。</p><p>3.这不仅仅是一套洞洞书。</p><p>猜猜我是谁探索游戏 '
                          '启蒙认知小百科 中英双语说话练习 '
                          '趣味动作语言模仿训练，一书多用，小惊喜里有大发现。</p><p>①趣味洞洞，玩不够。抠、摸、戳、挖、翻、看，锻炼精细动作，协调手眼脑，激发好奇，活跃思维。</p><p>②百科知识，轻松学。文字描述、图片展示既是猜谜线索，又是百科小知识。在阅读和游戏中，宝宝潜移默化地获得多元化、侵入化、场景化的认知训练和知识积累。</p><p>③中英双语，真容易。丰富的表达，大量的叠词、拟声词，简单的中英文短句和词语，激发宝宝的双语兴趣，扩展词汇量，刺激表达欲，抓住“语爆期”，促进宝宝语言能力飞跃。</p><p>④模仿游戏，好有趣。猜到答案后，再来玩模仿秀的游戏吧。标志性的叫声、样子、动作等着宝宝来学呢。有趣的互动，强化了认知；拟声、拟态，发展了语言和动作。</p><p>4.符合0～4岁好奇宝宝的探索需求。</p><p>尊重处于细微事物敏感期宝宝们的天性和特质，让宝宝在观察、探索、猜想、认知的过程中，拓展想象，主动思考，开发潜能。</p><p>5.图、文、音，立体阅读、多感官刺激。</p><p>高清情景大图及活泼生动的小配图，提升观察力、辨别力、认知力；富有互动性和节奏感的文字，让宝宝爱上读书、学会表达。</p><p>赠送配套中英双语音频，扫码即可无限畅听，叫醒小耳朵。听声猜物，锻炼宝宝记忆力和声音辨识能力；有声伴读，让宝宝学习和家长辅导都零负担。</p><p>6.猜猜找找，探索发现，模仿强化，玩一样的学习。</p><p>关注细节，把握线索，先找到蛛丝马迹，获取有效信息；再带着问题去思考，去揭秘，快乐探知。</p><p>1.DK</p><p>品牌，品质保证，高清大图，实物认知。</p><p>\n'
                          '</p><p>来自英国DK的双语互动玩具书，国际品牌，专业水准。</p><p>\n'
                          '</p><p>精美、真实的图片，更利于宝宝对未知世界形成正确的认知。</p><p>\n'
                          '</p><p>2.</p><p>宝宝酷爱的内容大集合。</p><p>\n'
                          '</p><p>萌宠来袭：《宠物小乖乖》《动物小可爱》； '
                          '车辆集结：《卡车，真能干》《拖拉机，帮帮忙》；动物出没：《农场伙伴玩游戏》《野生动物跑出来》。</p><p>\n'
                          '</p><p>3.</p><p>这不仅仅是一套洞洞书。</p><p>\n'
                          '</p><p>猜猜我是谁探索游戏 启蒙认知小百科 中英双语说话练习 '
                          '趣味动作语言模仿训练，一书多用，小惊喜里有大发现。</p><p>\n'
                          '</p><p>①趣味洞洞，玩不够。抠、摸、戳、挖、翻、看，锻炼精细动作，协调手眼脑，激发好奇，活跃思维。</p><p>\n'
                          '</p><p>②百科知识，轻松学。文字描述、图片展示既是猜谜线索，又是百科小知识。在阅读和游戏中，宝宝潜移默化地获得多元化、侵入化、场景化的认知训练和知识积累。</p><p>\n'
                          '</p><p>③中英双语，真容易。丰富的表达，大量的叠词、拟声词，简单的中英文短句和词语，激发宝宝的双语兴趣，扩展词汇量，刺激表达欲，抓住“语爆期”，促进宝宝语言能力飞跃。</p><p>\n'
                          '</p><p>④模仿游戏，好有趣。猜到答案后，再来玩模仿秀的游戏吧。标志性的叫声、样子、动作等着宝宝来学呢。有趣的互动，强化了认知；拟声、拟态，发展了语言和动作。</p><p>\n'
                          '</p><p>4.</p><p>符合0～4岁好奇宝宝的探索需求。</p><p>\n'
                          '</p><p>尊重处于细微事物敏感期宝宝们的天性和特质，让宝宝在观察、探索、猜想、认知的过程中，拓展想象，主动思考，开发潜能。</p><p>\n'
                          '</p><p>5.</p><p>图、文、音，立体阅读、多感官刺激。</p><p>\n'
                          '</p><p>高清情景大图及活泼生动的小配图，提升观察力、辨别力、认知力；富有互动性和节奏感的文字，让宝宝爱上读书、学会表达。</p><p>\n'
                          '</p><p>赠送配套中英双语音频，扫码即可无限畅听，叫醒小耳朵。听声猜物，锻炼宝宝记忆力和声音辨识能力；有声伴读，让宝宝学习和家长辅导都零负担。</p><p>\n'
                          '</p><p>6.</p><p>猜猜找找，探索发现，模仿强化，玩一样的学习。</p><p>\n'
                          '</p><p>关注细节，把握线索，先找到蛛丝马迹，获取有效信息；再带着问题去思考，去揭秘，快乐探知。</p><p>\n'
                          '</p><p>7.</p><p>亲子互动的好素材、好道具。</p><p>\n'
                          '</p><p>爸爸妈妈和宝宝在亲子共读和互动游戏中，亲子同乐，收获美好的陪伴时光；同时让宝宝积累知识，启蒙双语，实现互动式学习。</p><p>\n'
                          '</p><p>8.</p><p>高品质认知启蒙读物。</p><p>\n'
                          '</p><p>贴心圆角，加厚卡纸，双层对裱，耐撕、耐摔、耐咬，反复阅读，安全阅读。</p><p>\xa0'
                          '</p><p>显示全部信息</p>',
 'images': ['http://img3m7.ddimg.cn/30/3/27874767-1_u_19.jpg',
            'http://img3m7.ddimg.cn/30/3/27874767-2_u_5.jpg',
            'http://img3m7.ddimg.cn/30/3/27874767-3_u_6.jpg',
            'http://img3m7.ddimg.cn/30/3/27874767-4_u_6.jpg',
            'http://img3m7.ddimg.cn/30/3/27874767-5_u_6.jpg',
            'http://img3m7.ddimg.cn/30/3/27874767-6_u_6.jpg',
            'http://img3m7.ddimg.cn/30/3/27874767-7_u_6.jpg',
            'http://img3m7.ddimg.cn/30/3/27874767-8_u_3.jpg'],
 'isbn': '9787304097684',
 'media_review': '',
 'name': 'DK猜猜我是谁双语洞洞书（6册，赠中英双语音频）',
 'package': '平装-胶订',
 'paper': '铜版纸',
 'press': '国家开放大学出版社',
 'price': '228.00',
 'pub_date': '2019年09月',
 'sell_price': '205.20',
 'size': '24开',
 'suit': '是',
 'url': 'http://product.dangdang.com/27874767.html'}
2020-11-22 22:23:10 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 3 pages/min), scraped 1 items (at 1 items/min)
2020-11-22 22:24:10 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2020-11-22 22:24:29 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2020-11-22 22:24:29 [scrapy.core.engine] INFO: Closing spider (shutdown)
2020-11-22 22:24:29 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2233,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 74914,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2020, 11, 22, 14, 24, 29, 773341),
 'item_scraped_count': 1,
 'log_count/DEBUG': 5,
 'log_count/INFO': 13,
 'memusage/max': 57704448,
 'memusage/startup': 54788096,
 'request_depth_max': 2,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2020, 11, 22, 14, 22, 10, 350865)}
2020-11-22 22:24:29 [scrapy.core.engine] INFO: Spider closed (shutdown)
